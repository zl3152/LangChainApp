{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU openai\n",
        "%pip install -qU PyMuPDF\n",
        "%pip install -qU tiktoken"
      ],
      "metadata": {
        "id": "tIbXUgtMBmat"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "TKuU6hkOYqL0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "PS1szVxru1AW",
        "outputId": "63642f7d-f3a5-4d28-86fa-fadcf2371ea8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As an AI language model, I cannot advise or give guidance on how to pass interviews. However, here are some general tips that may help:\\n\\n1. Research the company and the job position you are applying for. Know their values, mission, and vision. This will help you prepare better for the interview.\\n\\n2. Prepare for common interview questions and practice your responses. This will help build your confidence and ensure that you can communicate effectively.\\n\\n3. Dress appropriately and arrive at the interview location at least 10 minutes early. Punctuality is important and demonstrates that you respect the interviewer's time.\\n\\n4. Listen attentively to the interviewer and make sure you answer their questions directly and concisely.\\n\\n5. Highlight your strengths, experience, and qualifications that match the job requirements.\\n\\n6. Be honest, respectful, and courteous throughout the interview.\\n\\n7. Finally, make sure you ask some thought-provoking and relevant questions about the job and the company. This shows that you are genuinely interested in the position and would like to learn more.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import openai\n",
        "openai.api_key =\"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\"\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\", \n",
        "  messages=[{\"role\": \"user\", \"content\": \"How to pass the interview?\"}]\n",
        ")\n",
        "completion['choices'][0]['message']['content']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz"
      ],
      "metadata": {
        "id": "yXMC2VmBEX6J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = fitz.open('439_multi_robot_scene_completion_t.pdf')\n",
        "text1 = \"\"\n",
        "for page in doc1:\n",
        " text1+=page.get_text()\n",
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "qDS3gidfBydC",
        "outputId": "3b30109e-aed6-4183-8106-9cfe09a2692c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multi-Robot Scene Completion:\\nTowards Task-Agnostic Collaborative Perception\\nYiming Li∗\\nNew York University\\nyimingli@nyu.edu\\nJuexiao Zhang∗\\nNew York University\\njuexiao.zhang@nyu.edu\\nDekun Ma\\nNew York University\\ndm4524@nyu.edu\\nYue Wang\\nMassachusetts Institute of Technology\\nyuewang@csail.mit.edu\\nChen Feng†\\nNew York University\\ncfeng@nyu.edu\\nAbstract: Collaborative perception learns how to share information among mul-\\ntiple robots to perceive the environment better than individually done. Past re-\\nsearch on this has been task-speciﬁc, such as detection or segmentation. Yet this\\nleads to different information sharing for different tasks, hindering the large-scale\\ndeployment of collaborative perception. We propose the ﬁrst task-agnostic col-\\nlaborative perception paradigm that learns a single collaboration module in a self-\\nsupervised manner for different downstream tasks. This is done by a novel task\\ntermed multi-robot scene completion, where each robot learns to effectively share\\ninformation for reconstructing a complete scene viewed by all robots. Moreover,\\nwe propose a spatiotemporal autoencoder (STAR) that amortizes over time the\\ncommunication cost by spatial sub-sampling and temporal mixing.\\nExtensive\\nexperiments validate our method’s effectiveness on scene completion and col-\\nlaborative perception in autonomous driving scenarios. Our code is available at\\nhttps://coperception.github.io/star/.\\nKeywords: Multi-Robot Perception, Scene Completion, Representation Learning\\n1\\nIntroduction\\nSingle robot perception has been widely studied on tasks such as object detection [1] and semantic\\nsegmentation [2]. However, it suffers from various challenges, such as occlusion and sparsity in\\nraw observations. Collaborative perception is promising to alleviate those issues. It provides more\\nenvironment observations from different perspectives by information sharing to improve perception\\nperformance and robustness. Amongst different collaboration strategies, feature-level collabora-\\ntion [3, 4, 5] transmits the intermediate representations generated by deep neural networks (DNNs)\\nof each robot. Since these intermediate features are easy to compress and can preserve contextual\\ninformation of the scene, feature-level collaboration demonstrates better performance-bandwidth\\ntrade-off compared to raw-data-level and output-level collaboration [6, 7].\\nHowever, existing feature-level collaboration methods [8, 4, 3] are fully supervised by task-speciﬁc\\nlosses to learn the entire model, including a feature extractor, a collaboration module, and a decoder,\\nas shown in Fig. 1 (a). Such a task-speciﬁc framework requires re-training the whole model for\\ndifferent perception tasks. Besides, existing collaborative perception requires training data record-\\nings to be synchronized among all robots in time, which is more demanding than data collection in\\nsingle-robot perception. How can we design a collaborative perception framework (1) independent\\nfrom downstream tasks and (2) trainable from asynchronous datasets?\\nTo answer this question, we propose a novel self-supervised learning task termed multi-robot scene\\ncompletion. It enables multiple robots to collaboratively use an autoencoder to reconstruct a com-\\nplete scene based on shared latent features. The completed scene could then be fed into various\\n∗indicates equal contribution.\\n†Corresponding author\\n6th Conference on Robot Learning (CoRL 2022), Auckland, New Zealand.\\n(a) \\nTask-specific \\nframework\\n(b) \\nTask-agnostic \\nframework\\nPerception \\nTask Loss\\nA Specific \\nPerception \\nTask\\nSingle View\\nFeature \\nExtractor\\nDecoder \\nand Output \\nHeader\\nSingle \\nView\\nEncoder\\nMulti-View\\nDecoder\\nMany Other\\nPerception Tasks\\n…\\nScene \\nCompletion\\nloss\\nFigure 1: Task-speciﬁc vs Task-agnostic collaboration. Task-speciﬁc paradigm learns different\\nmodels with different losses for each task. Whereas for the task-agnostic paradigm, reconstruction\\nof the multi-robot scene is learned, which is independent of yet still usable by all downstream tasks.\\ndownstream tasks without additional training, as shown in Fig. 1 (b). This allows us to decou-\\nple the collaboration training from downstream task learning. Moreover, it seamlessly supports\\nsynchronous and asynchronous training datasets with different learning objectives: complete scene\\nreconstruction if synchronous and individual view reconstruction if asynchronous.\\nYet naive autoencoders are not designed to balance scene reconstruction performance and communi-\\ncation volume, which is an established criterion to evaluate collaborative perception. To address this\\nchallenge, we further design a spatiotemporal autoencoder (STAR) inspired by the recent masked au-\\ntoencoders (MAE) [9]. It reconstructs a scene using a spatiotemporal mixture of patch tokens: some\\ntokens are encoded from randomly sub-sampled patches in the current frame and others are cached\\nfrom the past. The sampling ensures that all patches in the mixture can jointly cover the whole spa-\\ntial region while being self-disjoint. This allows each robot to only transmit the sub-sampled tokens\\nin the current frame instead of the entire latent feature maps, leading to much lower communication\\nbandwidth than prior works. Our key insight behind such an amortized communication cost is that\\nfeatures of many patches (e.g., static or nearly static) do not need to be shared in every frame.\\nIn summary, our main contributions are threefold:\\n• We propose a brand-new task-agnostic collaborative perception framework based on multi-robot\\nscene completion, decoupling the collaboration learning from downstream tasks.\\n• We propose asynchronous training and synchronous inference with a shared autoencoder to solve\\nthe proposed task, eliminating the need for synchronous data for collaboration learning.\\n• We develop a novel spatiotemporal autoencoder (STAR) that reconstructs scenes based on tempo-\\nrally mixed information. It amortizes the spatial communication volume over time to improve the\\nperformance-bandwidth trade-off.\\n• We conduct extensive experiments to verify our method’s effectiveness for scene completion and\\ndownstream perception in autonomous driving scenarios.\\n2\\nRelated Works\\nCollaborative perception. Collaborative perception has been proposed to improve individual per-\\nception’s ﬂexibility, resilience, and efﬁciency [10, 11, 12]. With recent advances in deep learning,\\nresearchers have developed feature-level collaborative perception in which intermediate representa-\\ntions produced by deep neural networks (DNNs) from multiple viewpoints are propagated in a team\\nof robots, e.g., a swarm of drones [8, 3] or a group of vehicles [4, 6]. Existing works commonly\\nconsider a speciﬁc downstream task and use the corresponding loss function to learn a collabora-\\ntion module such as a graph neural network (GNN) [4, 3], a Transformer [13, 14], and a convo-\\nlutional neural network [5, 15]. Several downstream tasks have been investigated in collaborative\\nscenarios, such as object detection [4], semantic segmentation [8], and depth estimation [3]. Un-\\nlike the existing task-speciﬁc collaborative perception sharing task-dependent representations, we\\ndeﬁne task-agnostic 3 collaborative perception as the feature-level collaborative perception sharing\\ntask-independent representations amongst multiple robots.\\n3Herein ”task” denotes the downstream perception task such as object detection and semantic segmentation.\\n2\\nScene completion. Autonomous navigation [16] requires robots to understand the geometry and\\nsemantics of 3D scenes. However, vision sensors only capture partial observations because of a\\nlimited ﬁeld of view and sparse sensing, leading to an incomplete spatial representation. Therefore,\\nscene completion (SC) has been proposed to infer the complete 3D scene geometry given sparse\\n2D/3D observations [17, 18, 19]. Following scene completion, semantic scene completion (SSC)\\nhas been introduced to jointly estimate both geometry and semantic information based on partial\\nobservation [2, 20, 21, 22]. On the one hand, single robot scene completion can rely on prior\\nsemantic knowledge to complete the partially-observed objects. On the other hand, it is unrealistic to\\nsee through full occlusions. Unlike single robot scene completion depending on prior knowledge, the\\nmulti-robot scene completion task utilizes information shared by teammates for scene completion.\\nSelf-supervised representation learning. Self-supervised representation learning (SSRL) aims to\\nprovide powerful features without the need for massive annotated datasets [23]. SSRL is generally\\ncomposed of: (1) task-agnostic pre-training via carefully-designed self-supervised pretext tasks such\\nas contrastive learning [24, 25] or autoencoding [26, 27, 9], and (2) task-speciﬁc adaptation to ﬁne-\\ntune the pre-trained model on the downstream tasks such as object detection or image classiﬁcation.\\nMasked autoencoder (MAE) achieves great performance with a simple reconstruction objective [9].\\nIt employs an asymmetric architecture with a large encoder that only processes unmasked patches\\nand a lightweight decoder that reconstructs the masked patches from the latent representation. Re-\\ncent works extend MAE into multimodal representation learning [28, 29], video [30, 31], and 2D\\nimage completion [32]. In this work, we employ similar autoencoding to learn the shared repre-\\nsentations and enable quick adaptation to the downstream perception: the reconstructions could be\\nseamlessly utilized by off-the-shelf individual perception model trained on single-view data without\\nany ﬁne-tuning, bridging the gap between collaborative perception and individual perception.\\n3\\nMulti-Robot Scene Completion: Motivation, Formulation, and Evaluation\\nMotivation. Even though single-robot data already requires arduous annotations like 3D bounding\\nboxes and pixel-wise semantic labeling, multi-robot data even demands multiple times as much\\nwork. To relax the task-dependent supervision for collaboration learning, we propose multi-robot\\nscene completion to enable task-agnostic collaborative perception. It can utilize self-supervision to\\nlearn shared representations instead of expensive task-dependent supervision. We will introduce its\\noverall workﬂow, training objective, and evaluation metrics hereafter.\\nProblem setup. We consider N robots in the same geographical location simultaneously perceiving\\nthe 3D environment, such as a ﬂeet of autonomous vehicles located at a crossroad. These robots\\ncommunicate with each other about their observations to better understand the surrounding envi-\\nronment. Each robot indexed by i is equipped with a 3D sensor such as a LiDAR to generate a\\nbinary occupancy grid map Mi ∈ {0, 1}H×W ×C deﬁned in its local coordinate, where H, W, and\\nC respectively denote the length, width, and height resolution.\\nFeature extraction. We employ intermediate collaboration with better performance-bandwidth\\ntrade-off [5]. Each robot encodes its observation into a feature map denoted by Fi = Θ(Mi),\\nwhere Θ is a feature extractor. Now Fi ∈ R ¯\\nH× ¯\\nW × ¯\\nC has lower spatial resolution ¯H × ¯W, while\\nkeeping a higher feature dimension ¯C compared to the original map Mi. Then, each robot will\\nbroadcast Fi to its peers as well as its pose ξi ∈ se(3) deﬁned in the global coordinate.\\nFeature decoding. The robot i receives the messages from the neighboring robots {Fj, ξj}j̸=i, and\\nthen uses a decoder Φ and a pose-aware aggregator A for fusion, and output a completed occupancy\\ngrid map ˆYi = Φ(A(Fi, ξi, {Fj, ξj}j̸=i)), where ˆYi has the same dimension and describe the\\nsame spatial range as Mi yet is a more comprehensive spatial representation for the scene. The\\npose-aware aggregator A transforms the feature maps of robot j(j ̸= i) into the coordinate of the\\ntarget robot i, then sums all the coordinate-synchronized feature maps:\\nA(Fi, ξi, {Fj, ξj}j̸=i) = Fi +\\nN−1\\n�\\nj=1\\nΓj→i(Fj),\\n(1)\\nwhere Γj→i ∈ SE(3) is the transformation from robot j’s coordinate to i’s, obtained by the expo-\\nnential map of poses ξj and ξi. Bi-linear interpolation is used to transform a discrete map, and the\\npositions out of the spatial range ¯H × ¯W after transformation are padded with zero.\\n3\\nTraining loss. We treat the scene completion task as a binary classiﬁcation problem and use cross-\\nentropy loss to train a neural network composed of Θ and Φ. Speciﬁcally, the ground-truth Yi ∈\\n{0, 1}H×W ×C deﬁned in the coordinate of robot i represents a multi-view occupancy voxel grid\\nwith two classes, i.e., free and occupied. Therefore, the loss can be computed by:\\nL = −\\nN−1\\n�\\ni=0\\nL−1\\n�\\nk=0\\n1\\n�\\nc=0\\nyi,k,clog(\\neˆyi,k,c\\n�\\nc eˆyi,k,c ),\\n(2)\\nwhere i is the robot index, k is the voxel index, L is the total number of the voxel (L = H ×W ×C),\\nc is the number of class (2 in our case), ˆyi,k,c is the predicted logits for the k-th voxel belonging\\nto class c, yi,k,c is the k-th element of Yi and is a one-hot vector (yi,k,c = 1 if voxel k of robot\\ni belongs to class c). Here we show the training objective using synchronous multi-robot data\\ncreated by aggregating multi-robot observations based on robots’ poses similar to Eq. (1): Yi =\\nA(Mi, ξi, {Mj, ξj}j̸=i) (j < M), where M is the number of robots for the ground truth generation.\\nNote that Θ and Φ can also be trained by individual view reconstruction on asynchronous data as\\nshown in Eq. (3), while being deployed on synchronous data for inference as discussed in Section 4.\\nHallucinating the invisible scene is possible if the ground-truth Yi is generated with more robots\\nthan those involved in the training phase, i.e., M >N. Yet only synchronous training can achieve\\nthis because the ground truth requires aggregation of different viewpoints obtained synchronously.\\nEvaluation metrics. We follow the evaluation protocol in single-robot scene completion [19, 33],\\nwhich uses the voxel-level intersection over union (IoU) between predicted voxel labels ˆYi and\\nground truth labels Yi for each robot. Note that only non-empty voxels are evaluated.\\n4\\nSTAR: Spatiotemporal Autoencoder\\nIn addition to the multi-robot scene completion task, we also propose a novel architecture called\\nSaptiotemporal autoencoder (STAR) to tackle this problem. We will present our key design moti-\\nvation, detailed modules, training, and inference procedures.\\n4.1\\nDesign rationale\\nPartially broadcasting. Inspired by the idea of ”masking” in MAE [9], we employ a similar asym-\\nmetric design as MAE yet with different purposes: MAE is to design a nontrivial self-supervisory\\ntask for pre-training via randomly masking, while the goal of STAR is to reduce the communica-\\ntion volume in multi-robot systems via partial broadcasting. More speciﬁcally, STAR deploys an\\nencoder at the sender robot to map the entire observation to an intermediate feature representation\\nthat is selectively transmitted to lower the bandwidth. Meanwhile, STAR deploys a decoder at the\\nreceiver robot that reconstructs the original observation from the received partial representation.\\nSpatiotemporal amortization. Simply applying random masking to a complete observation is not\\na good idea. Unlike MAE mainly for object-level recognition, we aim at large-scale dynamic scene\\nmodeling. Once objects are completely masked during encoding, the decoder cannot hallucinate the\\ncorresponding objects without such knowledge. Therefore, we propose to exploit historical tokens\\nto replace mask tokens during decoding. In this way, we can amortize the communication cost over\\nthe temporal domain by spatial sub-sampling and temporal mixing. Speciﬁcally, at the sender’s side,\\na part of the spatial patches are sampled from the entire observation in each timestamp. Only the\\nfeatures of these subsampled spatial patches are communicated with the teammates. Upon receiving\\nthe transmitted patch features, the receiver’s decoder combines these features with the historical\\npatch features and reconstructs an entire observation. Since the patches are subsampled in a spatially\\ncomplementary manner, the temporally mixed patches jointly cover the whole spatial region.\\nSynchronization-free training. Traditional collaborative perception approaches consider a syn-\\nchronization training strategy that requires synchronous (potentially with a slight temporal latency)\\nmulti-robot recordings to train a feature-space collaboration strategy with task-speciﬁc loss func-\\ntions [4, 5]. In this work, we try to relax the requirement that multiple robots simultaneously capture\\nperception data by using single-view observations as the supervision during the training phase. We\\nwill describe this in detail in section 4.3.\\n4\\n…\\nAsynchronous Training (Ours)\\nSynchronous Inference\\n…\\n𝑴𝒊,𝒕\\nCompleted \\nMultiview \\nScene\\nDec.\\nEnc.\\nSynchronous Training\\nTask \\nSpecific \\nLosses\\nScene \\nReconstruction \\nLoss\\nPose-aware \\nAggregation\\n𝑭𝒋,𝒕, 𝝃𝒋\\n𝑭𝒊,𝒕, 𝝃𝒊\\n𝑴𝒊\\n𝑴𝒋\\n𝑴𝒌\\nSingle-robot \\nPerception Model\\n𝑴𝒋,𝒕\\n𝑭𝒊,𝒕\\n𝑭𝒋,𝒕\\n𝑭𝒊,𝒕 𝑭𝒊,𝒕$𝟏\\n𝑭𝒋,𝒕 𝑭𝒋,𝒕$𝟏\\n𝑭𝒊,𝒕\\n𝑭𝒊,𝒕$𝟏\\n𝑭𝒋,𝒕\\n𝑭𝒋,𝒕$𝟏\\n𝝃𝒊\\n𝝃𝒋\\n𝑴𝒌,𝒕\\n𝑭𝒌,𝒕\\n𝕋\\n𝕋\\nEnc.\\nEnc.\\nDec.\\nEnc.\\nEnc.\\nDec.\\nEnc.\\nDec.\\nEnc.\\nDec.\\nDec.\\nTask Specific Outputs\\nEnc.\\nDec.\\nEnc.\\nDec.\\nS\\nS\\nS\\nFigure 2: Asynchronous training and synchronous inference. In the top right, asynchronous train-\\ning does not require communication between robots. In contrast, in the bottom right, synchronous\\ntraining requires communication and optimization w.r.t. each speciﬁc task loss. The synchronous\\ninference is illustrated on the left. The sender transmit encoded representations to the receiver.\\nThe receiver uses a mixture of spatiotemporal tokens to complete the scene observation. S: spatial\\nsub-sampling. T: temporal mixing.\\n4.2\\nArchitecture\\nWe consider a set of robots deploying the same neural network following [4, 5]. Each robot serves\\nas both message sender and receiver during collaboration and is equipped with an encoder for ob-\\nservation abstraction and a decoder for view reconstruction.\\nSTAR encoder. Different from MAE [9], the STAR encoder uses a vision transformer (ViT) [34]\\nbackbone, which operates on all patches yet only sends out a subset (spatial sub-sampling). Specif-\\nically, the entire grid map for robot i at time t denoted by Mi,t is divided into multiple patches,\\nand each patch is encoded with a linear projection with additional positional embedding, and then\\nprocessed using a series of Transformer blocks to generate the ﬁnal message Fi,t. Note that we\\nadopt a complementary transmission strategy in the temporal domain regarding the patch index (i.e.,\\nthe observed spatial locations) to avoid the loss of information for the dynamic scenes.\\nSTAR decoder. Different from MAE which uses mask tokens to replace the missed patch embed-\\ndings, robot i as a receiver aggregates the historic tokens Fj,t−1 and the current tokens Fj,t from\\nrobot j (temporal mixing), which approximately form a complete observation towards the entire spa-\\ntial range. Temporal embeddings are added to the tokens from the respective timestamps to enhance\\nthe temporal awareness before feeding them into Transformer blocks. Note that here we use two-\\ntimestamp t and t−1 as an example for a simple explanation. The STAR decoder is also able to pro-\\ncess more historical timestamps. After decoding all robots’ views { ˆMj,t}j̸=i, the ultimate prediction\\nof the complete view is computed by coordinate synchronization: ˆYi = A(Mi,t, ξi, { ˆMj,t, ξj}j̸=i),\\nand its calculation process is similar to Eq. (1).\\n4.3\\nTraining and inference\\nAsynchronous training. The model is trained with single view ground-truth Mi, and adopt cross-\\nentropy loss during training:\\nL = −\\nN−1\\n�\\ni=0\\nL−1\\n�\\nk=0\\n1\\n�\\nc=0\\nmi,k,clog(\\ne ˆmi,k,c\\n�\\nc e ˆmi,k,c ),\\n(3)\\nwhere i is the robot index, k is the voxel index, L is the total number of the voxels, c = 2 is\\nnumber of class, mi,k,c denotes the k-th element of Mi and is a one-hot vector same as yi,k,c in\\nEq. 2, ˆmi,k,c is the prediction for the k-th voxel belonging to class c. Note that the training loss\\nis calculated voxel-wise with respect to the self-supervision signal from each robot’s single-view\\nobservation. This design decouples the training phase from communication with other robots: the\\nmodel on each robot does not require synchronous observations from neighbor robots in the training\\n5\\nphase, making the training asynchronous (asynchronous training in Fig. 2). This is greatly different\\nfrom the training framework in previous collaborative perception works such as [5] (synchronous\\ntraining in Fig. 2). Our training framework can relax the need for the carefully-collected and hard-\\nto-annotate multi-robot dataset and can exploit a large amount of single-robot data to learn powerful\\nas well as compact feature representations.\\nSynchronous inference. During inference, each robot is equipped with the same model. The sender\\nrobots’ encoders will encode and broadcast a subset of their current timestamp’s observation. Then,\\nthe decoders on the receiver side will leverage the transmitted intermediate representation along\\nwith the pose information to reconstruct the corresponding view, optionally with historical features\\nas described above. Then, the receivers use corresponding pose information to aggregate the single\\nobservations into a multi-view completed scene. We illustrate the pipeline on the left side of Fig. 2.\\n5\\nExperimental Results\\n5.1\\nExperimental setup\\nDataset. We conduct experiments on the V2X-Sim Dataset [15], a large-scale dataset that simulates\\nurban multi-vehicle driving scenes with CARLA [35]. We use 80 scenes for training and ten scenes\\nfor testing. The dataset is sampled at 5 Hz. We pre-process the voxels grids with range [−32m, 32m]\\nin the x and y-axis and [−3m, 2m] in the z-axis. Finally, we can get the voxel grids with a spatial\\nresolution of 256 × 256 × 13.\\nBaselines. The lower-bound refers to a single-robot perception model trained and tested using\\nonly individual observations. The task-speciﬁc models all optimize the collaboration based on\\nthe speciﬁc perception head. When2com [8] uses the attention mechanism to fuse the collaborators’\\ninformation. Who2com [36] employs a handshake mechanism. The V2VNet [4] trains a graph\\nneural network to propagate the agent’s information. DiscoNet [5] selectively fuse messages from\\nthe informative regions. For task-agnostic models, we use a modiﬁed FaFNet [37] backbone as the\\nCNN baseline and substitute the detection head with a classiﬁcation head that outputs the logits for\\nbinary classiﬁcation. VQ-VAE [26] learns a variational autoencoder to reconstruct the scene and\\nemploys a vector quantization technique to reduce communication costs.\\nImplementation details. A 6-block ViT encoder with hidden dimension 384 is used for the STAR\\nencoder. Then an MLP is used to compress the intermediate representations to 32 dimensions and\\nfeed them to the decoder, where they are projected back to 256 dimensions and sent to a 4-layer\\ntransformer decoder. An FaFNet [37] is used for single-robot object detection. A UNet [38] serves\\nthe same purpose for the semantic segmentation task. Note that all the perception models take the\\nthree-dimensional voxel grids as input and output results in bird’s eye view (BEV), such as bounding\\nboxes and semantic labels. Our models are all trained on single-view data.\\nEvaluation metrics. For the scene completion task, we measure the completion quality using the\\nintersection-over-union (IoU) at three different scales by down-sampling the voxels accordingly.\\nFor the perception task, we report the average precision (AP) at thresholds 0.5 and 0.7 for vehicle\\ndetection, IoU for the vehicle category, and the overall mIoU for semantic segmentation.\\n5.2\\nQuantitative results on scene completion\\nWe present the quantitative results of the multi-robot scene completion task in Table 1, measured by\\nIoU at different scales and the corresponding communication bandwidth.\\nSpatial resolution. Among the three tested resolutions, we can see that, in general, a higher spatial\\nresolution leads to a better completion quality: the spatial resolution 32 × 32 which has a patch size\\nof 8 achieves the best performance.\\nTimestamps. Our method allows the multi-robot system to amortize the spatial communication\\nbandwidth over the temporal domain. From Table 1, we can see that from timestamps 1 to 4, the\\nperformance only varies slightly while largely reducing the bandwidth.\\nBandwidth. Bandwidth is calculated to reﬂect the required data volume for communication per\\nsecond. A trade-off between performance and communication is clear. The CNN baseline requires\\nmuch higher bandwidth because multiple feature maps are transmitted during communication due\\n6\\nTimestamp\\nIoU scale 1:1\\nIoU scale 1:2\\nIoU scale 1:4\\nCommunication Bandwidth\\n32x32\\n16x16\\n8x8\\n32x32\\n16x16\\n8x8\\n32x32\\n16x16\\n8x8\\n32x32\\n16x16\\n8x8\\nSTAR TS1\\n55.13\\n53.11\\n50.79\\n77.40\\n72.16\\n66.55\\n83.28\\n79.30\\n73.33\\n1.3MB/s\\n320.0KB/s\\n80.0KB/s\\nSTAR TS2\\n54.93\\n52.07\\n50.40\\n75.71\\n69.63\\n64.86\\n82.51\\n76.24\\n70.81\\n640.0KB/s\\n160.0KB/s\\n40.0KB/s\\nSTAR TS3\\n53.35\\n51.56\\n50.39\\n72.52\\n68.19\\n64.75\\n79.20\\n74.51\\n70.55\\n427.0KB/s\\n106.7KB/s\\n26.7KB/s\\nSTAR TS4\\n53.65\\n51.64\\n49.69\\n72.98\\n68.15\\n63.39\\n79.73\\n74.36\\n68.98\\n320.0KB/s\\n80.0 KB/s\\n20.0KB/s\\nCNN backbone\\n55.37\\n77.17\\n83.51\\n155.0MB/s\\nTable 1: Quantitative results on scene completion. Results across different spatial resolutions and\\ntimestamps are presented. Note TSX means fusing temporal information across X TimeStamps.\\nTimestamp\\nAll\\nPartial\\n1\\n65.19\\n-\\n2\\n64.68\\n61.36\\n3\\n64.53\\n63.77\\n(a) Patches to encode. All: encodes all. Partial:\\nonly encodes those patches being transmitted.\\nTimestamp\\nMulti\\nSingle\\n1\\n65.19\\n-\\n2\\n64.68\\n52.07\\n3\\n64.53\\n51.97\\n(b) Timestamps to decode. For timestamp 1, de-\\ncoding a single timestamp is equivalent to multi.\\nTimestamp\\nTemporal Emb.\\nw/\\nw/o\\n2\\n64.68\\n64.29\\n3\\n64.53\\n61.83\\n(c) Temporal embedding. W/ means temporal\\nembeddings are added. W/o means not.\\nStrategy\\nTimestamp\\n2(50%)\\n3(66%)\\n4(75%)\\nrandom\\n50.88\\n52.36\\n52.14\\ncompl.\\n64.45\\n64.20\\n63.27\\n(d) Masking strategy. The ratio of random mask-\\ning is set equivalent to complementary masking.\\nTable 2: Ablation studies. The performance is reported in IoU 1:2 for the spatial resolution 16x16.\\nThe observations under other settings are consistent.\\nto the skip connections in the model. STAR requires much lower bandwidth. A ﬁner-grained spatial\\nresolution with better performance requires a higher bandwidth.\\n5.3\\nAblation studies on scene completion\\nWe conduct several ablation studies to investigate the effectiveness of the key components in our\\nmethod. Results are presented in Table 2 and are discussed in detail below.\\nPatches to encode. As shown in Table 2a, only encoding the patches that will be transmitted can re-\\nsult in a minor drop in performance. Yet it can reduce some computations for computation-restricted\\nrobotic systems.\\nTimestamps to decode. We investigate the effect of whether the decoder incorporates previous\\ntimestamps or just the current single timestamp combined with learnable mask tokens. Results in\\nTable 2b indicate that historical information is essential.\\nTemporal embedding. In the STAR decoder, we add temporal embedding to the patches of different\\ntimestamps, similar to the approach in [30, 31]. The ablation study in Table 2c shows that adding\\ntemporal embedding is beneﬁcial.\\nMasking strategy. We compared our complementary masking strategy with the random masking\\nstrategy proposed in MAE [9] in Table 2d. Results show that switching from complementary to\\nrandom masking leads to a degradation in the completion performance.\\n5.4\\nQuantitative results on downstream perception\\nWe directly feed the single-view to the single-robot perception model termed lower-bound without\\nany ﬁne-tuning, and the results are shown in Table 3. Our best STAR method improves the lower-\\nbound by 25.9% and 22.8% in object detection (AP@IoU=0.7) and semantic segmentation (IoU\\nof vehicle) respectively. Achieved by simply combining the completion model with off-the-shelf\\nsingle-robot perception models, these improvements are promising because our framework: (1) has\\nno knowledge about downstream tasks (task-agnostic); (2) does not require synchronous data in the\\ntraining phase (synchronization-free); (3) is learned without manual annotations (self-supervised).\\nWe also investigated the single-robot perception model directly taking ground truth multi-view mea-\\nsurements without additional training, termed upper-bound, and ﬁnd that it can achieve nearly com-\\n7\\nParadigm\\nMethod\\nDetection\\nSemantic Segmentation\\nAP@IoU=0.5\\nAP@IoU=0.7\\nVehicle\\nmIoU\\nSingle-robot perception\\nLower-bound\\n49.90\\n44.21\\n45.93\\n36.64\\nTask-speciﬁc multi-robot perception\\nWhen2com [8]\\n44.02\\n39.89\\n47.87\\n34.49\\nWho2com [36]\\n44.02\\n39.89\\n47.84\\n34.49\\nV2VNet [4]\\n68.35\\n62.83\\n58.35\\n41.17\\nDiscoNet [5]\\n69.03\\n63.44\\n55.84\\n41.34\\nTask-agnostic multi-robot perception\\nSTAR TS1\\n62.84\\n57.22\\n56.41\\n39.09\\nSTAR TS2\\n61.48\\n55.75\\n56.13\\n38.97\\nVQ-VAE\\n60.27\\n54.08\\n55.40\\n38.48\\nCNN baseline\\n59.85\\n54.05\\n54.61\\n38.32\\nUpper-bound\\n65.09\\n60.26\\n60.34\\n40.45\\nTable 3: Quantitative results on downstream tasks. The task-speciﬁc methods achieve excellent\\nresults via elaborate supervised learning with synchronous multi-robot recordings. The task-agnostic\\nmethods use single-robot perception models with reconstructed observations.\\nparable performance with DiscoNet [5] and V2VNet [4], both trained with full supervision using\\nsynchronous data for speciﬁc tasks. This demonstrates the potential of our proposed task: when the\\ncompletions approach the ground truth scenes, it can perform similarly to the upper bound on many\\ndownstream tasks. Moreover, using a stronger single-robot perception model can further enhance\\nthe ﬁnal performance.\\n5.5\\nQualitative results on scene completion and downstream perception\\nWe present a few qualitative results on the scene completion and the downstream tasks in Figure 3.\\nDue to the limited space, more visualizations can be found in the appendix.\\nTrue Observation \\nCompleted Observation\\nTrue Segmentation \\nPredicted Segmentation\\nDifference\\nTrue Observation \\nTrue Observation \\nCompleted Observation\\nCompleted Observation\\nDetection\\n(a) Completion\\n(b) Detection\\n(C) Semantic Segmentation\\nFigure 3: Qualitative results. (a), (b) and (c) each presents a qualitative example of the scene comple-\\ntion, detection, and segmentation tasks respectively. Refer to the appendix for more visualizations.\\n6\\nLimitation\\nThere is still a performance gap between our method and the upper bound on the downstream per-\\nception due to the non-perfect scene completion. We believe when trained with more single-robot\\nrecordings, our method is able to achieve comparable performance to task-speciﬁc approaches while\\nmaintaining excellent ﬂexibility. Currently the spatial tokens are sub-sampled randomly at individ-\\nual timestamp, and we believe this could be improved in the future. We also inherit the common\\nlimitation in most existing collaborative perception works: all experiments are on simulated datasets\\ndue to the lack of public real-world datasets. We further ignore the inﬂuence of pose noises, although\\nprevious works [5] already revealed reasonable robustness.\\n7\\nConclusion\\nWe propose the ﬁrst task-agnostic collaborative perception paradigm, where a single collaboration\\nmodule is learned and can be transferred to different downstream tasks. Our key observation is that\\nwe can move communication between robots to the temporal domain, which achieves an excellent\\nperformance-bandwidth trade-off. Also, our self-supervised learning method sheds new light on\\ncollaborative perception that reduces the importance of human annotations.\\n8\\nAcknowledgments\\nWe thank the anonymous reviewers for their valuable comments in revising this paper. This work\\nwas supported by the NSF CPS Program under Grant CMMI-1932187 and CNS-2121391.\\nReferences\\n[1] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao, and J. Solomon. Detr3d: 3d object\\ndetection from multi-view images via 3d-to-2d queries. In Conference on Robot Learning,\\npages 180–191. PMLR, 2021.\\n[2] R. Cheng, C. Agia, Y. Ren, X. Li, and L. Bingbing. S3cnet: A sparse semantic scene comple-\\ntion network for lidar point clouds. In Conference on Robot Learning, 2020.\\n[3] Y. Zhou, J. Xiao, Y. Zhou, and G. Loianno. Multi-robot collaborative perception with graph\\nneural networks. IEEE Robotics and Automation Letters, 2022.\\n[4] T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and R. Urtasun. V2vnet: Vehicle-\\nto-vehicle communication for joint perception and prediction. In Proceedings of the European\\nConference on Computer Vision (ECCV), pages 605–621, 2020.\\n[5] Y. Li, S. Ren, P. Wu, S. Chen, C. Feng, and W. Zhang. Learning distilled collaboration graph\\nfor multi-agent perception. In Advances in Neural Information Processing Systems, volume 34,\\n2021.\\n[6] Q. Chen, S. Tang, Q. Yang, and S. Fu. Cooper: Cooperative perception for connected au-\\ntonomous vehicles based on 3d point clouds. In IEEE International Conference on Distributed\\nComputing Systems (ICDCS), pages 514–524, 2019.\\n[7] E. Arnold, M. Dianati, R. de Temple, and S. Fallah. Cooperative perception for 3d object\\ndetection in driving scenarios using infrastructure sensors. IEEE Transactions on Intelligent\\nTransportation Systems, 2020.\\n[8] Y.-C. Liu, J. Tian, N. Glaser, and Z. Kira. When2com: multi-agent perception via communi-\\ncation graph grouping. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 4106–4115, 2020.\\n[9] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, and R. Girshick. Masked autoencoders are scalable vi-\\nsion learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\\n2022.\\n[10] Y. Hu, S. Fang, Z. Lei, Y. Zhong, and S. Chen. Where2comm: Communication-efﬁcient collab-\\norative perception via spatial conﬁdence maps. In Advances in neural information processing\\nsystems, 2022.\\n[11] Z. Lei, S. Ren, Y. Hu, W. Zhang, and S. Chen. Latency-aware collaborative perception. In\\nProceedings of the European Conference on Computer Vision (ECCV), 2022.\\n[12] S. Su, Y. Li, S. He, S. Han, C. Feng, C. Ding, and F. Miao. Uncertainty quantiﬁcation of\\ncollaborative detection for self-driving. arXiv preprint arXiv:2209.08162, 2022.\\n[13] R. Xu, Z. Tu, H. Xiang, W. Shao, B. Zhou, and J. Ma. Cobevt: Cooperative bird’s eye view\\nsemantic segmentation with sparse transformers. In 6th Annual Conference on Robot Learning.\\n[14] R. Xu, H. Xiang, Z. Tu, X. Xia, M.-H. Yang, and J. Ma. V2x-vit: Vehicle-to-everything\\ncooperative perception with vision transformer. In Proceedings of the European Conference\\non Computer Vision (ECCV), 2022.\\n[15] Y. Li, D. Ma, Z. An, Z. Wang, Y. Zhong, S. Chen, and C. Feng. V2x-sim: Multi-agent col-\\nlaborative perception dataset and benchmark for autonomous driving.\\nIEEE Robotics and\\nAutomation Letters, 7(4):10914–10921, 2022.\\n9\\n[16] S. Garg, N. S¨underhauf, F. Dayoub, D. Morrison, A. Cosgun, G. Carneiro, Q. Wu, T.-J. Chin,\\nI. Reid, S. Gould, et al. Semantics for robotic mapping, perception and interaction: A survey.\\nFoundations and Trends® in Robotics, 8(1–2):1–224, 2020.\\n[17] J. Davis, S. R. Marschner, M. Garr, and M. Levoy. Filling holes in complex surfaces using\\nvolumetric diffusion. In Proceedings. First International Symposium on 3D Data Processing\\nVisualization and Transmission, pages 428–441. IEEE, 2002.\\n[18] M. Firman, O. Mac Aodha, S. Julier, and G. J. Brostow. Structured prediction of unobserved\\nvoxels from a single depth image. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 5431–5440, 2016.\\n[19] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic scene com-\\npletion from a single depth image. In Proceedings of the IEEE conference on computer vision\\nand pattern recognition, pages 1746–1754, 2017.\\n[20] L. Roldao, R. de Charette, and A. Verroust-Blondet. Lmscnet: Lightweight multiscale 3d\\nsemantic completion. In 2020 International Conference on 3D Vision (3DV), pages 111–119.\\nIEEE, 2020.\\n[21] C. B. Rist, D. Emmerichs, M. Enzweiler, and D. M. Gavrila. Semantic scene completion using\\nlocal deep implicit functions on lidar data. IEEE transactions on pattern analysis and machine\\nintelligence, 44(10):7205–7218, 2021.\\n[22] X. Yan, J. Gao, J. Li, R. Zhang, Z. Li, R. Huang, and S. Cui. Sparse single sweep lidar point\\ncloud segmentation via learning contextual shape priors from scene completion. In Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages 3101–3109, 2021.\\n[23] L. Ericsson, H. Gouk, C. C. Loy, and T. M. Hospedales. Self-supervised representation learn-\\ning: Introduction, advances, and challenges. IEEE Signal Processing Magazine, 39(3):42–62,\\n2022.\\n[24] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\\nrepresentation learning. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 9729–9738, 2020.\\n[25] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola. What makes for good views\\nfor contrastive learning? In Advances in Neural Information Processing Systems, volume 33,\\npages 6827–6839, 2020.\\n[26] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. In Advances in\\nneural information processing systems, volume 30, 2017.\\n[27] D. P. Kingma and M. Welling.\\nAuto-encoding variational bayes.\\narXiv preprint\\narXiv:1312.6114, 2013.\\n[28] R. Bachmann, D. Mizrahi, A. Atanov, and A. Zamir.\\nMultimae: Multi-modal multi-task\\nmasked autoencoders. arXiv preprint arXiv:2204.01678, 2022.\\n[29] X. Geng, H. Liu, L. Lee, D. Schuurams, S. Levine, and P. Abbeel. Multimodal masked autoen-\\ncoders learn transferable representations. arXiv preprint arXiv:2205.14204, 2022.\\n[30] Z. Tong, Y. Song, J. Wang, and L. Wang. Videomae: Masked autoencoders are data-efﬁcient\\nlearners for self-supervised video pre-training. In Advances in neural information processing\\nsystems, 2022.\\n[31] C. Feichtenhofer, H. Fan, Y. Li, and K. He. Masked autoencoders as spatiotemporal learners.\\nIn Advances in neural information processing systems, 2022.\\n[32] C. Zheng, T.-J. Cham, and J. Cai. Tﬁll: Image completion via a transformer-based architecture.\\narXiv preprint arXiv:2104.00845, 2021.\\n[33] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall. Se-\\nmantickitti: A dataset for semantic scene understanding of lidar sequences. In Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision, pages 9297–9307, 2019.\\n10\\n[34] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-\\nhghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transform-\\ners for image recognition at scale. In International Conference on Learning Representations,\\n2020.\\n[35] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun. Carla: An open urban driving\\nsimulator. In Conference on robot learning, pages 1–16. PMLR, 2017.\\n[36] Y.-C. Liu, J. Tian, C.-Y. Ma, N. Glaser, C.-W. Kuo, and Z. Kira. Who2com: Collaborative\\nperception via learnable handshake communication. In 2020 IEEE International Conference\\non Robotics and Automation (ICRA), pages 6876–6883, 2020.\\n[37] W. Luo, B. Yang, and R. Urtasun. Fast and furious: Real time end-to-end 3d detection, tracking\\nand motion forecasting with a single convolutional net. In Proceedings of the IEEE conference\\non Computer Vision and Pattern Recognition, pages 3569–3577, 2018.\\n[38] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical im-\\nage segmentation. In International Conference on Medical image computing and computer-\\nassisted intervention, pages 234–241. Springer, 2015.\\n11\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1=pd.DataFrame(['0',text1]).T\n",
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "YCeFTREAYwO5",
        "outputId": "918cf7a7-67ca-490f-fb32-ffc4d209acca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0                                                  1\n",
              "0  0  Multi-Robot Scene Completion:\\nTowards Task-Ag..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af4526bc-29f5-4bab-a4ac-240613e59466\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Multi-Robot Scene Completion:\\nTowards Task-Ag...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af4526bc-29f5-4bab-a4ac-240613e59466')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af4526bc-29f5-4bab-a4ac-240613e59466 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af4526bc-29f5-4bab-a4ac-240613e59466');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "#df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
        "df1.columns = ['title', 'text']\n",
        "\n",
        "# Tokenize the text and save the number of tokens to a new column\n",
        "df1['n_tokens'] = df1.text.apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "# Visualize the number of tokens in the entire article \n",
        "df1.n_tokens.hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "LJTRHkA3Y4O7",
        "outputId": "c11b8627-5c8e-44e3-b61e-1afc63536afd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = fitz.open('V2X-SIM.pdf')\n",
        "text2 = \"\"\n",
        "for page in doc2:\n",
        " text2+=page.get_text()\n",
        "text2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "m2NVhP41YQiy",
        "outputId": "0a2cefe5-ac3c-4452-a064-af9845348a7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. JULY, 2022\\n1\\nV2X-Sim: Multi-Agent Collaborative Perception\\nDataset and Benchmark for Autonomous Driving\\nYiming Li1, Dekun Ma1, Ziyan An1, Zixun Wang1, Yiqi Zhong2, Siheng Chen3, u, and Chen Feng1, u\\nAbstract—Vehicle-to-everything (V2X) communication tech-\\nniques enable the collaboration between vehicles and many other\\nentities in the neighboring environment, which could fundamen-\\ntally improve the perception system for autonomous driving.\\nHowever, the lack of a public dataset signiﬁcantly restricts the\\nresearch progress of collaborative perception. To ﬁll this gap,\\nwe present V2X-Sim, a comprehensive simulated multi-agent\\nperception dataset for V2X-aided autonomous driving. V2X-\\nSim provides: (1) multi-agent sensor recordings from the road-\\nside unit (RSU) and multiple vehicles that enable collaborative\\nperception, (2) multi-modality sensor streams that facilitate\\nmulti-modality perception, and (3) diverse ground truths that\\nsupport various perception tasks. Meanwhile, we build an open-\\nsource testbed and provide a benchmark for the state-of-the-\\nart collaborative perception algorithms on three tasks, including\\ndetection, tracking and segmentation. V2X-Sim seeks to stimulate\\ncollaborative perception research for autonomous driving before\\nrealistic datasets become widely available. Our dataset and code\\nare available at https://ai4ce.github.io/V2X-Sim/.\\nIndex Terms—Deep learning for visual perception, multi-robot\\nsystems, data sets for robotic vision.\\nI. INTRODUCTION\\nP\\nERCEPTION is a fundamental capability for autonomous\\nvehicles, which allows them to represent, identify, and\\ninterpret sensory input for understanding the complex sur-\\nroundings. In literature, single-vehicle perception has been\\nintensively studied thanks to the well-established driving\\ndatasets [1]–[3], and researchers have proposed various algo-\\nrithms to deal with different downstream tasks [4]–[6].\\nDespite recent advances in single-vehicle perception, the\\nindividual viewpoint often results in degraded perception in\\nlong-range or occluded areas. A promising solution to this\\nproblem is through vehicle-to-everything (V2X) [7], a cutting-\\nedge communication technology that enables dialogue be-\\ntween a vehicle and other entities, including vehicle-to-vehicle\\nManuscript received February 24, 2022; Revised June 7, 2022; Accepted\\nJune 30, 2022.\\nThis paper was recommended for publication by Editor Cesar Cadena upon\\nevaluation of the Associate Editor and Reviewers’ comments. This work was\\nsupported by the NSF CPS program under CMMI-1932187, the National\\nNatural Science Foundation of China under Grant 62171276, the Science and\\nTechnology Commission of Shanghai Municipal under Grant 21511100900\\nand CALT Grant 2021-01.\\nu Corresponding author.\\n1Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, and Chen Feng are with are\\nwith New York University, Brooklyn, NY 11201, USA cfeng@nyu.edu\\n2Yiqi Zhong is with University of Southern California, Los Angeles, USA\\nyiqizhon@usc.edu\\n3Siheng Chen is with Cooperative Medianet Innovation Center, Shang-\\nhai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China\\nsihengc@sjtu.edu.cn\\nDigital Object Identiﬁer (DOI): see top of this page.\\nFig. 1: (a) Intersection for vehicle-to-everything (V2X) communication. (b) Workﬂow of\\nmulti-agent collaborative perception with intermediate-/feature-based strategy. We bench-\\nmark collaborative object detection, multi-object tracking, and semantic segmentation in\\nthe bird’s eye view (BEV).\\n(V2V) and vehicle-to-infrastructure (V2I). With the aid of\\nV2X communication, we are able to upgrade single-vehicle\\nperception to collaborative perception, which introduces more\\nviewpoints to help autonomous vehicles see further, better and\\neven see through occlusion, thereby fundamentally enhancing\\nthe capability of perception.\\nCollaborative perception naturally draws on communication\\nand perception. Its development requires expertise from both\\ncommunities. Recently, the communication community has\\nmade enormous efforts to promote such a study [8]–[10];\\nhowever, only a few works have been proposed from the\\nperspective of perception [11]–[15]. One major reason for\\nthis is the lack of well-designed and organized collaborative\\nperception datasets. Due to the immaturity of V2X and the\\ncost of simultaneously operating multiple autonomous vehi-\\ncles, it is very expensive and laborious to build such a real\\ndataset for research communities. Therefore, we synthesize a\\ncomprehensive and publicly available dataset, named as V2X-\\nSim, to advance the study of collaborative perception for V2X-\\ncommunication-aided autonomous driving.\\nTo generate V2X-Sim, we employ SUMO [16], a micro-\\ntrafﬁc simulation, to produce numerically-realistic trafﬁc ﬂow,\\nand CARLA [17], a widely-used open-source simulator for\\nautonomous driving research, to retrieve well-synchronized\\nsensor streams from multiple vehicles as well as the road-\\nside unit (RSU). Meanwhile, multi-modality sensor streams\\nof different entities are recorded to enable cross-modality per-\\nception. In addition, diverse annotations including bounding\\nboxes, vehicle trajectories, and semantic labels are provided\\nto facilitate various downstream tasks. To better serve multi-\\nagent, multi-modality, and multi-task perception research for\\nautonomous driving, we further provide a benchmark for three\\ncrucial perception tasks (collaborative detection, tracking, and\\narXiv:2202.08449v2  [cs.CV]  16 Jul 2022\\n2\\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. JULY, 2022\\nTABLE I\\nCOMPARISON OF COLLABORATIVE PERCEPTION DATASETS FOR AUTONOMOUS\\nDRIVING. THERE ARE NO PUBLIC DATASETS WHICH SUPPORT BOTH V2V AND V2I\\nRESEARCH: MULTI-AGENT DATA ARE EITHER GENERATED BY SIMULATORS [12],\\n[20] OR CREATED BY SELECTING CONSECUTIVE FRAMES FROM SINGLE-AGENT REAL\\nDATASETS [11], [21], [22]. SEVERAL WORKS COLLECT DATA FROM MULTIPLE\\nINFRASTRUCTURE SENSORS: [23] IN SIMULATION, [24], [25] IN REAL WORLD. OUR\\nDATASET IS THE FIRST PUBLIC MULTI-AGENT MULTI-MODALITY DATASET WHICH\\nSUPPORTS DIFFERENT COLLABORATIVE PERCEPTION TASKS.\\nDataset\\nScenario\\nSource\\nSensor\\nTasks\\nPublic\\nRGBDepthLiDARIMU/GPS Det.Track.Seg.\\nV2V-Sim [12]\\nV2V\\nLiDARsim [26]\\n\\x17\\n\\x17\\n\\x13\\n\\x17\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\nCooper [11]\\nV2V\\nKITTI [2]\\n\\x17\\n\\x17\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\nMFSL [21]\\nV2V\\nKITTI [2]\\n\\x13\\n\\x17\\n\\x17\\n\\x13\\n\\x17\\n\\x17\\n\\x13\\n\\x17\\nVANETs [22]\\nV2V\\nKITTI [2]\\n\\x13\\n\\x17\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nOPV2V [20]\\nV2V\\nOpenCDA [27] & CARLA [17]\\n\\x13\\n\\x17\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x13\\nCODD [28]\\nV2V\\nCARLA [17]\\n\\x17\\n\\x17\\n\\x13\\n\\x17\\n\\x13\\n\\x13\\n\\x17\\n\\x13\\nCooper (inf) [23]\\nV2I\\nCARLA [17]\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\n\\x13\\n\\x13\\n\\x17\\n\\x13\\nWIBAM [24]\\nV2I\\nReal-world\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\n\\x13\\n\\x13\\n\\x17\\n\\x13\\nDAIR-V2X [25]\\nV2I\\nReal-world\\n\\x13\\n\\x17\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\n\\x13\\nV2X-Sim (Ours) V2V&V2I\\nCARLA [17] & SUMO [16]\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\nsegmentation) on the proposed dataset using the state-of-the-\\nart collaboration strategies [12], [13], [18], [19]. In summary,\\nour contributions are two-fold:\\n• We propose V2X-Sim, a comprehensive V2X perception\\ndataset for autonomous driving, to support multi-agent\\nmulti-modality multi-task perception research.\\n• We create an open-source testbed for collaborative per-\\nception methods, and provide a benchmark on three tasks\\nto encourage more research in this ﬁeld.\\nII. RELATED WORK\\nAutonomous driving dataset. Since the pioneering dataset\\nKITTI [2] was released, the autonomous driving community\\nhas been trying to increase the dataset comprehensiveness in\\nterms of driving scenarios, sensor modalities, and data an-\\nnotations. Regarding driving scenarios, current datasets cover\\ncrowded urban scenes [29], adverse weather conditions [30],\\nnight scenes [31], and multiple cities [1] to enrich the data dis-\\ntribution. As for sensor modalities, nuScenes [1] collects data\\nwith Radar, RGB cameras, and LiDAR in a 360◦ viewpoint;\\nWoodScape [32] captures data with ﬁsheye cameras. Regard-\\ning data annotations, semantic labels in both images [33]–[36]\\nand point clouds [37], [38] are provided to enable semantic\\nsegmentation; 2D/3D box trajectories are offered [39], [40]\\nto facilitate tracking and prediction. In summary, most real\\ndatasets emphasize the data comprehensiveness in single-\\nvehicle situations, but ignore the multi-vehicle scenarios.\\nV2X system and dataset. By sharing information with\\nother vehicles or the RSU, V2X mitigates the shortcom-\\nings of single-vehicle perception and planning such as the\\nlimited sensing range and frequent occlusion [7]. Previous\\nresearch [41] has developed an enhanced cooperative mi-\\ncroscopic trafﬁc model in V2X scenarios, and studied the\\neffect of V2X in trafﬁc disturbance scenarios. [42] proposes a\\nmulti-modal cooperative perception system that provides see-\\nthrough, lifted-seat, satellite and all-around views to drivers.\\nMore recently, COOPER [11] investigates raw-data level\\ncollaborative perception to improve the detection capability\\nfor autonomous driving. V2VNet [12] proposes intermediate-\\nfeature level collaboration to promote the vehicle’s perception\\nTABLE II\\nSENSOR SUITE OF VEHICLE (V) AND\\nINTERSECTION (I).\\nSensor\\nDescription\\nV: 6 × RGB cam-\\nera I: 4 × RGB\\ncamera\\nEach vehicle is equipped with 6 cameras. Each\\ncamera has a FoV of 70◦, except for the back\\ncamera that has a FoV of 110◦. Each RSU has\\n4 cameras looking diagonally downward at 35◦\\nwith a 70◦ FoV. The image size is 1600×900.\\nV: 6 × Depth cam-\\nera I: 4 × Depth\\ncamera\\nEach vehicle has 6 depth cameras with the same\\nsetting as RGB cameras. Each RSU has 4 depth\\ncameras with the same setting as its RGB cameras.\\nV: 6 × Semantic\\ncamera I: 4 × Se-\\nmantic camera\\nEach vehicle has 6 semantic segmentation cameras\\nwith the same setting as RGB cameras. Each RSU\\nhas 4 semantic segmentation cameras with the\\nsame setting as its RGB cameras.\\nV&I: 1 × BEV se-\\nmantic camera\\nEach vehicle and RSU has one BEV semantic\\ncamera at the top, looking downward. Both the\\nraw images (semantic tags encoded in the red\\nchannel) and the converted colored images are\\nprovided. The image size is 900×900.\\nV&I: 1 × LiDAR\\nand\\nSemantic\\nLi-\\nDAR\\nWe attach one LiDAR and one semantic LiDAR\\non top of the ego vehicle and the intersection cen-\\nter. Specs: 32 channels, 70m max range, 250,000\\npoints per second, 20 Hz rotation frequency.\\nFig. 2: Sensor layout and coordinate sys-\\ntems.\\nand prediction capability. Several works use multiple infras-\\ntructure sensors to jointly perceive the environment and em-\\nploy output-level collaboration with vehicle-to-infrastructure\\ncommunication [23], [24]. As for the datasets,\\n[11], [21],\\n[22] simulate the V2V scenarios with different frames from\\nKITTI [2]. Yet, these datasets are unrealistic multi-agent\\ndatasets for the measurements are not captured by different\\nviewpoints. Some other works use a platoon strategy for\\ndata capture [43], [44], but they are biased because the\\nobservations were highly correlated with each other. The most\\nrelevant work is V2V-Sim [12], which is based on a high-\\nquality LiDAR simulator [26]. Unfortunately, V2V-Sim does\\nnot include the V2I scenario and is not publicly available.\\nMoreover, OPV2V [20] and CODD [28] only support the\\ndetection task in the V2V scenario. Existing collaborative\\nperception datasets are summarized in Table I: V2X-Sim1 is\\ncurrently the most comprehensive one with multi-agent multi-\\nmodality sensory streams in both V2V and V2I scenarios, and\\ncan support various downstream tasks such as multi-agent\\ncollaborative detection, tracking, and semantic segmentation.\\nIII. V2X-SIM DATASET\\nV2X-Sim could enable more research on the collaboration\\nstrategy among vehicles to achieve a more robust perception.\\nThis could fundamentally beneﬁt autonomous driving, intelli-\\ngent transportation systems, smart cities, etc..\\nA. Sensor suite on vehicles and RSU\\nMulti-modality data is essential for robust perception. To\\nensure the comprehensiveness of our dataset, we equip each\\nvehicle with a sensor suite composed of RGB cameras, Li-\\nDAR, GPS, IMU, and RSU with RGB cameras and LiDAR.\\nSensor conﬁguration. On both the vehicle and RSU, the\\ncamera and LiDAR cover 360◦ horizontally to enable full-\\nview perception. Speciﬁcally, each vehicle carries six RGB\\ncameras following the nuScenes conﬁguration [1]; the RSU is\\nequipped with four RGB cameras toward four directions at the\\ncrossroad. We employ depth camera, semantic segmentation\\ncamera, semantic LiDAR, and BEV semantic segmentation\\ncamera in CARLA to obtain the corresponding ground-truth\\n1This work extends the LiDAR-based V2V data in our previous work [13]\\nwith more modalities, scenarios and downstream tasks.\\nLI et al.: V2X-SIM: MULTI-AGENT COLLABORATIVE PERCEPTION DATASET AND BENCHMARK FOR AUTONOMOUS DRIVING\\n3\\nFig. 3: (a) Visualizations of the bird’s eye view point cloud from different scenes. Gray denotes the point cloud captured by the RSU LiDAR. Each color (except for gray) represents\\na vehicle, and the orange boxes denote the vehicles in the scene. (b) Statistics for car bounding box sizes. (c) Counts for annotations per keyframe where the annotated vehicles are\\npresented within 0-30m, 30-50m, and 50-70m of the ego vehicles. (d) Counts for LiDAR points per annotation. (e) Statistics of the distance between every two ego vehicles for all\\nframes. (f) Speed of cars located within 70m from ego vehicles. (g) Percentage of annotated vehicles observed by 1-5 ego vehicles.\\nfor each RGB camera. Note that the BEV semantic segmenta-\\ntion camera is based on orthogonal projection while the ego-\\nvehicle semantic segmentation camera uses perspective pro-\\njection. Table II summarizes the detailed sensor speciﬁcation.\\nSensor layout and coordinate system. The overall sensor\\nlayout and coordinate system is shown in Fig. 2. The BEV\\nsemantic camera shares the same x, y position with LiDAR\\nyet is placed higher to ensure a certain size of ﬁeld of view.\\nNote that we invert the y-axis in CARLA and use a right-hand\\ncoordinate system following nuScenes [1].\\nDiverse annotations. To assist downstream tasks including\\ndetection, tracking and semantic segmentation, we provide\\nvarious annotations such as 3D bounding boxes, pixel-wise\\nand point-wise semantic labels. Each box is deﬁned by the\\nlocation of its center in x, y, z coordinates, and its width,\\nlength, and height. In total, there are 23 categories such as the\\npedestrian, building, ground, etc. In addition, precise depth\\nvalues are provided for depth estimation.\\nB. CARLA-SUMO co-simulation\\nWe consider it a realistic V2X scenario when multiple ve-\\nhicles with their own routes are simultaneously located in the\\nsame intersection. Each intersection is also equipped with one\\nRSU with sensing capability. Regarding the trafﬁc simulation,\\nthere are several non-public simulators which can explicitly\\ngenerate data tailored for collaborative perception such as\\nscenes with occlusion, and sensor range limitations [45],\\n[46]. Yet in this work, we use open-source CARLA-SUMO\\nco-simulation for trafﬁc ﬂow simulation and data recording.\\nVehicles are spawned in CARLA via SUMO to roam around\\nin the town with random routes. Hundreds of vehicles are\\nspawned in different towns (Town03, Town04 and Town05\\nthat have crossroads as well as T-junctions in both the crowded\\ndowntown and suburb highway). We record several log ﬁles\\nin different towns. Afterwards, at different junctions, we read\\nout 100 scenes from the log ﬁles. Each scene has a 20-\\nsecond duration, and we choose M(M = 2, 3, 4, 5) vehicles\\nas well as one RSU in each scene as intelligent agents to share\\ninformation. Example scenarios are shown in Fig. 3 (a).\\nC. Downstream tasks\\nOur dataset not only supports single-agent perception tasks\\nsuch as 3D object detection, tracking, image-/point-cloud-\\nbased semantic segmentation, depth estimation, but also en-\\nables collaborative perception such as collaborative 3D object\\ndetection, tracking, and collaborative BEV semantic segmen-\\ntation in urban driving scenes. We provide a benchmark for\\ncollaborative perception algorithms.\\nD. Dataset statistics\\nAnnotation statistics. We provide statistics on the annota-\\ntions and objects to highlight the inclusiveness and diversity\\nof our dataset. In Fig. 3 (b) we analyze the size of the cars’\\nbounding boxes within a 70m range from ego vehicles in\\neach frame. The great variation of car sizes indicates that our\\nscenes contain a diverse set of car makes and models that well\\nincludes most of the common real-world vehicles. Figure 3 (c)\\nshows the annotation count in each frame for vehicles within\\n0-30m, 30-50m, and 50-70m ranges from each ego vehicle. It\\nsuggests that our dataset features both crowded scenes (up to\\n100 annotations within 50-70m from the ego vehicle) and less\\ncrowded driving scenarios (as low as 10 annotations within\\n30m from the ego vehicle). Figure 3 (d) contains statistics\\non the number of LiDAR points per annotation for single-\\nagent and multi-agent scenarios respectively. The number of\\ntotal LiDAR points of each object annotation increases when\\nthere are more than one agents observing the same object.\\nSpeciﬁcally, for a single agent, there are 183.83 points in each\\nannotation on average, but the number goes up to 875.59 points\\nper annotation for multiple agents.\\nScene features. We analyze the distance between each two\\nego vehicles for every frame, as shown in Fig. 3 (e). An\\noverwhelming percentage of the ego vehicles are presented\\nwithin 20-30 meters from each other, suggesting they are\\ngeographically closely connected. The speed of cars within\\n70m from ego vehicles are shown in Fig. 3 (f). Given the fact\\nthat our scenes are selected near intersections, we notice that a\\nmajor fraction of vehicles are slower than 10 km/h. However,\\nthe maximum speed is as high as 90+ km/h. Figure 3 (g) shows\\n4\\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. JULY, 2022\\nthe percentage of annotations observed by a certain number\\nof ego vehicles, up to 5. Over 60% of the annotations are\\nobserved by at least two ego vehicles.\\nIV. COLLABORATIVE PERCEPTION BENCHMARK\\nWe benchmark three crucial perception tasks in autonomous\\ndriving within the collaboration setting: detection, tracking,\\nand semantic segmentation. The three tasks have been exten-\\nsively studied since they generate essential perception knowl-\\nedge for autonomous vehicles to make safer decisions. For per-\\nformance evaluation, we follow the same evaluation protocol\\nof the three tasks in the single-agent scenario, except that we\\nutilize the information shared by other agents while the single-\\nagent perception do not have access to such information. We\\nreport the results in two scenarios: (1) V2V only, and (2) V2X\\n(V2V + V2I).\\nDataset format and split. Our V2X-Sim dataset follows the\\nsame storage format of nuScenes [1], an authoritative multi-\\nmodality single-agent autonomous driving dataset. nuScenes\\ncollected real-world single-agent driving data to promote the\\nsingle-agent autonomous driving research; while we simulate\\nmulti-agent scenarios to facilitate the next-generation V2X-\\naided collaborative perception technology. Each scene in our\\ndataset contains a 20 second trafﬁc ﬂow at a certain inter-\\nsection of three CARLA towns, and the multi-modality multi-\\nagent sensory streams are recorded at 5Hz, meaning each scene\\nis composed of 100 frames. We generate 100 scenes with a\\ntotal of 10,000 frames, and in each scene, 2-5 vehicles are\\nselected as the collaboration agents. We use 8,000/1,000/1,000\\nframes for training/validation/testing respectively, and we en-\\nsure that there is no overlap in terms of the intersections across\\ntraining/validation/testing set. Each frame has data sampled\\nfrom multiple agents (vehicles and RSU). There are 37,200\\nsamples in the training set, 5,000 samples in the validation set,\\nand 5,000 samples in the test set. The split is shared across\\ntasks.\\nImplementation details. Bird’s-eye-view (BEV) is a widely\\nused and powerful representation in autonomous driving be-\\ncause it can describe the surrounding objects and overall\\ncontext via a compact top-down 2D map [47]. Therefore,\\nBEV-based representation is adopted in all three tasks: we\\nuse a 3D voxel grid to represent the 3D world, employ\\nbinary representation, and assign each voxel a positive label\\nif the voxel contains point cloud data. Since the gener-\\nated 3D voxel grid can be considered as a pseudo-image\\nwhose height dimension is the channel dimension, we can\\nperform the efﬁcient 2D convolution instead of the heavy\\n3D convolution. Speciﬁcally, we crop the points located in\\nthe region of [−32, 32] × [−32, 32] × [−3, 2] meters for\\nvehicles deﬁned in the ego-vehicle Euclidean coordinate and\\n[−32, 32] × [−32, 32] × [−8, −3] meters for RSU. The width\\nand length of each voxel are 0.25 meter, and the height is 0.4\\nmeter, meaning the generated BEV-based pseudo-image has\\na dimension of 256 × 256 × 13 (W × L × H). Note that the\\nmodels in all the tasks consume the 3D BEV map and generate\\nperception results in 2D BEV.\\nBenchmark models. We aim to benchmark collaborative\\nperception strategies rather than the well-studied individual\\nperception methods. We consider early/intermediate/late/no\\ncollaboration\\nmodels\\nfor\\nthe\\nbenchmark.\\nThe\\ninterme-\\ndiate\\nmodels,\\nincluding\\nDiscoNet\\n[13],\\nV2VNet\\n[12],\\nWhen2com [18], and Who2com [19], are based on the com-\\nmunication of the intermediate features of the neural network.\\nThe methods in our benchmark are as follows:\\n• Lower-bound: The single-agent perception model with-\\nout collaboration which processes a single-view point\\ncloud is considered as the lower-bound.\\n• Co-lower-bound: Collaborative lower-bound fuses the\\noutput from different single-agent perception models.\\n• Upper-bound: The early collaboration model which\\ntransmits raw point cloud data is the upper-bound.\\n• DiscoNet [13]: DiscoNet uses a directed collaboration\\ngraph with matrix-valued edge weight to adaptively high-\\nlight the informative spatial regions and reject the noisy\\nregions of the messages sent by the partners. After\\nadaptive message fusion, the updated features will be\\ntransmitted to the output head for perception.\\n• V2VNet [12]: V2VNet uses a pose-aware graph neural\\nnetwork to propagate agents’ information, and employs\\na convolutional gated recurrent unit based module to\\naggregate other agents’ information. After several rounds\\nof neural message passing, the updated features are fed\\ninto the output head to generate perception results.\\n• When2com [18]: When2com employs attention-based\\nmechanism for communication group construction: the\\npartners with satisfactory correlation scores will be se-\\nlected as the collaborators. After the attention-score-based\\nweighted fusion, the updated features will be fed into\\nthe output head for perceptions. The model with pose\\ninformation is marked by ∗.\\n• Who2com [19]: Who2com shares a similar idea with\\nWhen2com, yet it uses handshake mechanism to select\\nthe collaborator: the partner with the highest score will\\nbe selected as the collaborator. The model with pose\\ninformation is marked by ∗.\\nWe implement a 3D perception pipeline that can be in-\\ntegrated with all of the communication methods mentioned\\nabove. Since the source codes of V2VNet is not publicly\\navailable, we re-implement the V2VNet in PyTorch according\\nto its pseudo-code. For when2com/who2com, we borrow their\\ncommunication modules from its ofﬁcial code. All of the\\nintermediate collaboration modules use the same architecture\\nand conduct collaboration at the same intermediate feature\\nlayer. Moreover, all of the methods are trained with the same\\nsetting to ensure that the performance gain comes from the\\ncollaboration instead of irrelevant techniques.\\nA. Collaborative object detection in BEV\\nProblem deﬁnition. As the most crucial perception task in\\nautonomous driving, 3D object detection aims to recognize\\nand localize the objects in 3D scenes given a single frame,\\nwith the following tracking, prediction and planning modules\\nall heavily relying on the detections. The models consume\\nvoxelized point cloud and output BEV bounding boxes.\\nLI et al.: V2X-SIM: MULTI-AGENT COLLABORATIVE PERCEPTION DATASET AND BENCHMARK FOR AUTONOMOUS DRIVING\\n5\\nFig. 4: Visualizations of BEV detection on V2X-Sim. Red and green boxes are the predictions and ground-\\ntruths respectively.\\nTABLE III\\nQUANTITATIVE RESULTS OF COLLABORATIVE BEV DETECTION\\nON V2X-SIM. RSU DENOTES THE ROAD-SIDE UNIT. AP\\nDENOTES THE AVERAGE PRECISION. ∆ IS THE ABSOLUTE GAIN\\nIN AP INTRODUCED BY RSU.\\nMethod\\nAP@IoU=0.5\\nAP@IoU=0.7\\nw/o RSUw/ RSU\\n∆\\nw/o RSUw/ RSU\\n∆\\nLower-bound\\n49.90\\n46.96 ↓ 2.94\\n44.21\\n42.33 ↓ 1.88\\nCo-lower-bound\\n43.99\\n42.98 ↓ 1.01\\n39.10\\n38.26 ↓ 0.84\\nWhen2com [18]\\n44.02\\n46.39 ↑ 2.37\\n39.89\\n40.32 ↑ 0.43\\nWhen2com* [18]\\n45.35\\n48.28 ↑ 2.93\\n40.45\\n41.43 ↑ 0.68\\nWho2com [19]\\n44.02\\n46.39 ↑ 2.37\\n39.89\\n40.32 ↑ 0.43\\nWho2com* [19]\\n45.35\\n48.28 ↑ 2.93\\n40.45\\n41.13 ↑ 0.68\\nV2VNet [12]\\n68.35\\n72.08 ↑ 3.73\\n62.83\\n65.85 ↑ 3.02\\nDiscoNet [13]\\n69.03\\n72.87 ↑ 3.84\\n63.44\\n66.40 ↑ 4.81\\nUpper-bound\\n70.43\\n77.08 ↑ 6.65\\n67.04\\n72.57 ↑ 5.53\\nTABLE IV\\nQUANTITATIVE RESULTS OF BEV TRACKING ON V2X-SIM. MOTA: MULTIPLE OBJECT TRACKING ACCURACY. MOTP: MULTIPLE OBJECT TRACKING PRECISION. HOTA:\\nHIGHER ORDER TRACKING ACCURACY. DETA: DETECTION ACCURACY. ASSA: ASSOCIATION ACCURACY. DETRE: DETECTION RECALL. DETPR: DETECTION PRECISION.\\nASSRE: ASSOCIATION RECALL. ASSPR: ASSOCIATION PRECISION. LOCA: LOCALIZATION ACCURACY. THE NUMBER TO THE LEFT OF () DENOTES THE PERFORMANCE IN\\nV2V SOLELY. THE NUMBER IN () REPRESENTS THE PERFORMANCE GAIN BY ADDING V2I.\\nMethod\\nMOTA\\nMOTP\\nHOTA\\nDetA\\nAssA\\nDetRe\\nDetPr\\nAssRe\\nAssPr\\nLocA\\nLower-bound\\n35.72 (↓3.87)\\n84.16 (↓0.74)\\n34.27 (↓1.68)\\n33.64 (↓3.24)\\n36.18 (↓0.06)\\n35.07 (↓3.54)\\n82.49 (↑0.96)\\n46.70 (↑0.23)\\n58.72 (↑0.10)\\n86.43 (↑0.38)\\nCo-lower-bound\\n21.53 (↑0.58)\\n85.76 (↑0.15)\\n39.16 (↓0.71)\\n41.14 (↓0.93)\\n38.18 (↓0.62)\\n59.54 (↓2.52)\\n54.68 (↑0.79)\\n50.92 (↓0.65)\\n55.78 (↑0.84)\\n87.64 (↑0.38)\\nWhen2com [18]\\n29.48 (↑2.45)\\n86.10 (↓2.79)\\n30.94 (↑1.01)\\n27.90 (↑2.04)\\n35.33 (↑0.06)\\n28.67 (↑2.58)\\n86.11 (↓4.81)\\n46.30 (↓0.15)\\n59.20 (↓0.36)\\n87.98 (↓1.98)\\nWhen2com∗ [18]\\n30.17 (↑1.43)\\n84.95 (↓1.44)\\n31.34 (↑0.43)\\n29.11 (↑1.05)\\n35.42 (↑0.21)\\n30.28 (↑1.32)\\n83.81 (↑0.29)\\n46.65 (↓0.29)\\n58.61 (↑0.18)\\n86.14 (↑0.17)\\nWho2com [19]\\n29.48 (↑2.46)\\n86.10 (↓2.79)\\n30.94 (↑1.01)\\n27.90 (↑2.04)\\n35.33 (↑0.06)\\n28.67 (↑2.58)\\n86.11 (↓4.81)\\n46.30 (↓0.15)\\n59.20 (↓0.36)\\n87.98 (↓1.98)\\nWho2com∗ [19]\\n30.17 (↑1.43)\\n84.95 (↓1.44)\\n31.34 (↑0.43)\\n29.11 (↑1.06)\\n35.42 (↑0.21)\\n30.28 (↑1.33)\\n83.81 (↑0.29)\\n46.65 (↓0.29)\\n58.61 (↑0.18)\\n86.14 (↑0.17)\\nV2VNet [12]\\n55.29 (↑2.29)\\n85.21 (↓0.53)\\n43.68 (↑0.91)\\n50.71 (↑1.93)\\n38.76 (↑0.24)\\n53.40 (↑2.51)\\n84.45 (↓1.07)\\n50.22 (↑0.53)\\n58.50 (↓0.07)\\n87.22 (↑0.38)\\nDiscoNet [13]\\n56.69 (↑2.26)\\n86.23 (↓0.41)\\n44.76 (↑1.09)\\n52.41 (↑2.18)\\n39.25 (↑1.11)\\n54.87 (↑2.58)\\n86.29 (↓0.95)\\n50.86 (↑1.02)\\n58.94 (↓0.15)\\n88.07 (↑0.34)\\nUpper-bound\\n58.00 (↑3.92)\\n85.61 (↑0.25)\\n44.83 (↑4.24)\\n52.94 (↑4.24)\\n38.95 (↓0.75)\\n55.07 (↑4.68)\\n86.54 (↓0.30)\\n50.35 (↓0.86)\\n58.71 (↑0.15)\\n87.48 (↑0.06)\\nBackbone and evaluation. We use a classic anchor-based\\ndetector composed of a convolutional encoder, a convolutional\\ndecoder, and an output header for classiﬁcation and regres-\\nsion [48]. Regarding the loss function, we use the binary cross-\\nentropy loss to supervise the box classiﬁcation and the smooth\\nL1 loss to supervise the box regression, following [48]. We\\nemploy the generic BEV detection evaluation metric: Average\\nPrecision (AP) at Intersection-over-Union (IoU) thresholds of\\n0.5 and 0.7. We target the vehicle detection and report the\\nresults on the test set.\\nQuantitative results. Table III demonstrates the quantitative\\ncomparisons on AP (@IoU = 0.5/0.7). We ﬁnd that: (1)\\nthe upper-bound performs best amongst all methods, and\\nit improves lower-bound signiﬁcantly by 41.1% and 51.6%\\nin terms of AP@0.5 and AP@0.7 in the scenario of V2V\\nonly, validating the necessity of collaboration; (2) V2V and\\nV2I jointly can generally improve the perception over V2V\\nonly with more viewpoints, e.g., adding V2I can bring an\\nimprovement of 9.4% for upper-bound, and 5.5% for V2VNet\\nin terms of AP@0.5; (3) among the intermediate models,\\nDiscoNet achieves the best performance via the well-designed\\ndistilled collaboration graph, V2VNet achieves the second best\\nperformance via the powerful neural message passing, and\\nWhen2com/Who2com only achieve comparable performance\\nwith lower-bound since the attention-mechanism is not suit-\\nable in point-cloud-based collaborative perception: the agents\\nusually need complementary information rather than a highly\\nsimilar one; (4) the late collaboration model (co-lower-bound)\\nhurts the detection performance because of introducing extra\\nfalse positives from other vehicles.\\nQualitative results. The qualitative results are shown in\\nFig. 4. We see that the collaboration can fundamentally\\nmitigate the problems of long-range perception and occlusion.\\nB. Collaborative multi-object tracking in BEV\\nProblem deﬁnition. Different from detection, multi-object\\ntracking requires the generation of temporally consistent per-\\nception results. Multi-object tracking in BEV is to use bound-\\ning boxes, object categories, and object identities to track\\ndifferent objects within a temporal sequence.\\nEvaluation metrics. We mainly utilize HOTA (Higher\\nOrder Tracking Accuracy) [49] to evaluate our BEV track-\\ning performance. HOTA can evaluate detection, association,\\nand localization performance via a single uniﬁed metric. In\\naddition, the classic multi-object tracking accuracy (MOTA)\\nand multi-object tracking precision (MOTP) [50] are also em-\\nployed. MOTA can measure detection errors and association\\nerrors. MOTP solely measures localization accuracy.\\nBaseline tracker. We implement SORT [51] as our baseline\\ntracker. Given the detection results, SORT will combine the\\nKalman Filter and Hungarian algorithm to achieve an accurate\\nand efﬁcient tracking performance.\\nQuantitative results. Quantitative comparisons of BEV\\ntracking are shown in Table IV. Similar to BEV detection,\\nupper-bound achieves the best performance in terms of MOTA\\nand HOTA. Meanwhile, adding V2I can improve MOTA\\nlargely yet cannot help too much in MOTP. Co-lower-bound\\nshows good performance in localization accuracy (MOTP). A\\n6\\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. JULY, 2022\\nFig. 5: Visualizations of collaborative BEV semantic segmentation.\\nTABLE V\\nQUANTITATIVE RESULTS OF BEV SEGMENTATION ON V2X-SIM. THE NUMBER TO THE LEFT OF () DENOTES THE PERFORMANCE IN V2V SOLELY. THE NUMBER IN ()\\nREPRESENTS THE PERFORMANCE GAIN BY ADDING V2I.\\nMethod\\nVehicle\\nSidewalk\\nTerrain\\nRoad\\nBuilding\\nPedestrian\\nVegetation\\nmIoU\\nLower-bound\\n45.93 (↑2.22)\\n42.39 (↓2.75)\\n47.03 (↑0.20)\\n65.76 (↓1.27)\\n25.38 (↓1.89)\\n20.59 (↓3.09)\\n35.83 (↑0.66)\\n36.64 (↓0.87)\\nCo-lower-bound\\n47.67 (↑2.43)\\n48.79 (↓1.41)\\n50.92 (↑0.85)\\n70.00 (↓0.65)\\n25.26 (↑0.17)\\n10.78 (↓1.77)\\n39.46 (↑2.69)\\n38.38 (↑0.46)\\nWhen2com [18]\\n48.43 (↑0.03)\\n33.06 (↑1.38)\\n36.89 (↑1.76)\\n57.74 (↑1.56)\\n29.20 (↑1.18)\\n20.37 (↑0.57)\\n39.17 (↓0.01)\\n34.49 (↑0.88)\\nWhen2com∗ [18]\\n47.74 (↑1.23)\\n33.60 (↓0.40)\\n35.81 (↑1.05)\\n56.75 (↑0.48)\\n26.11 (↓0.92)\\n19.16 (↑0.04)\\n39.64 (↓2.55)\\n33.81 (↓0.47)\\nWho2com [19]\\n48.40 (↑0.06)\\n32.76 (↑1.68)\\n36.04 (↑2.61)\\n57.51 (↑1.79)\\n29.17 (↑1.21)\\n20.36 (↑0.58)\\n39.08 (↑0.08)\\n34.31 (↑1.06)\\nWho2com∗ [19]\\n47.74 (↑1.23)\\n33.60 (↓0.40)\\n35.81 (↑1.05)\\n56.75 (↑0.48)\\n26.11 (↓0.92)\\n19.16 (↑0.04)\\n39.64 (↓2.55)\\n33.81 (↓0.47)\\nV2VNet [12]\\n58.42 (↑3.09)\\n48.33 (↓3.87)\\n48.51 (↓1.59)\\n70.02 (↑0.46)\\n28.58 (↑5.18)\\n21.99 (↑0.57)\\n41.42 (↑0.35)\\n41.11 (↑0.74)\\nDiscoNet [13]\\n56.66 (↑1.19)\\n46.98 (↓1.74)\\n50.22 (↓1.05)\\n68.62 (↓0.25)\\n27.36 (↑5.58)\\n22.02 (↑0.82)\\n42.50 (↑0.95)\\n40.84 (↑0.53)\\nUpper-bound\\n64.09 (↑5.34)\\n41.34 (↑2.42)\\n48.20 (↑0.74)\\n67.05 (↑2.04)\\n29.07 (↑0.74)\\n31.54 (↑3.15)\\n45.04 (↑0.70)\\n42.29 (↑1.98)\\nmore advanced tracker is required to exploit the collaboration\\nfor ﬁlling the performance gap.\\nC. Collaborative semantic segmentation in BEV\\nProblem deﬁnition. We aim to conduct semantic segmen-\\ntation in BEV using only geometry point cloud. In the collab-\\norative perception scenarios, there are measurements collected\\nby multiple agents with distinct viewpoints. Therefore, there\\nare more information in the scene, facilitating the semantic\\nscene understanding.\\nBaseline segmentation method and evaluation metrics.\\nWe follow the backbone architecture as well as the loss\\nfunction of U-Net [52] in our baseline method. The input is\\na BEV-based voxelized point cloud, and the output is BEV\\nsemantic segmentation. We label and predict seven categories\\nas listed in Table V, while the remaining is unlabeled. In our\\nbenchmark, we evaluate the segmentation performance using\\nmean Intersection-over-Union (mIoU).\\nQuantitative results. As illustrated in Table V, we ﬁnd that:\\n(1) V2VNet, DiscoNet, and upper-bound achieve comparable\\nperformance in terms of terrain and road categories; (2) the\\nattention-based methods (i.e., when2com, who2com) performs\\nworse because the attention-based mechanisms try to ﬁnd the\\ncollaboration partners with high correlation scores. Whereas,\\nin 3D perception, the collaborators with complementary in-\\nformation should be prioritized during the collaboration. Such\\nnonalignment can make it quite hard for the attention model to\\nlearn; (3) there is a large gap between lower-bound and upper-\\nbound regarding the two safety-critical categories: vehicle\\n(45.93% v.s. 64.09%) and pedestrian (20.59% v.s. 31.54%),\\nproving the values of collaboration; (4) employing V2V and\\nV2I jointly can generally enhance the vehicle segmentation\\nover using V2V solely; (5) co-lower-bound performs better\\nthan the lower-bound.\\nQualitative results. Figure 5 shows the semantics segmen-\\ntation results. The results of upper-bound restore the semantic\\ninformation with rich point cloud data. Intermediate-based\\ncollaboration strategies V2VNet and DiscoNet can achieve\\nsatisfactory performance yet When2com and Who2com hurt\\nthe performance compared to the lower-bound.\\nD. Discussions on pose noise and compression ratio\\nWe further examine the robustness of different intermediate\\nmodels against realistic pose noise (Gaussian noise with a\\nmean of 0.05m−0.25m and a standard deviation of 0.02cm),\\nas shown in Fig. 6. We can see that the models perform com-\\nparably with or without pose noise in the training phase, and\\nall the intermediate methods have shown stable performance\\nagainst the pose noise. The reason is that the intermediate\\nfeature map has a relatively low spatial resolution (each grid\\nin the feature map denotes a coverage of 2m × 2m), thus\\nis not vulnerable to noisy pose. Meanwhile, we employ an\\nLI et al.: V2X-SIM: MULTI-AGENT COLLABORATIVE PERCEPTION DATASET AND BENCHMARK FOR AUTONOMOUS DRIVING\\n7\\nFig. 6: Experimental results of a robustness test conducted under various magnitudes of pose noise. AUG. means augmentation which adds pose noise during training.\\nFig. 7: Experimental results of a bandwidth test conducted under various compression ratios.\\n1 × 1 autoencoder to further compress the feature channel of\\nthe transmitted feature map. We test the models with different\\ncompression ratios, and we found that the jointly learned 1×1\\nautoencoder can even improve the performance a little bit, and\\nmost intermediate models achieve comparable performance at\\ndifferent levels of compression, as shown in Fig. 7.\\nV. CONCLUSION\\nWe propose V2X-Sim dataset based on CARLA-SUMO co-\\nsimulation, in order to enable multi-agent collaborative per-\\nception research in autonomous driving. Our dataset provides\\nmulti-agent multi-modality sensor streams captured by the\\nvehicles and road-side unit (RSU) in realistic trafﬁc ﬂows.\\nDiverse annotations are provided to support a variety of 3D\\nperception tasks. In addition, we benchmark several state-of-\\nthe-art collaborative perception methods in collaborative BEV\\ndetection, tracking, and semantic segmentation tasks. Future\\nworks include the simulation of latency issues as well as\\nthe development of novel evaluation metrics in collaborative\\nperception tasks. We believe our work can inspire many rel-\\nevant research areas including but not limited to autonomous\\ndriving, computer vision, multi-robot system, communication\\nengineering, and machine learning.\\nAcknowledgment. The authors would like to thank anony-\\nmous reviewers for their helpful suggestions, and NYU high\\nperformance computing (HPC) for the support.\\nREFERENCES\\n[1] H. Caesar, V. Bankiti, A. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,\\nY. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset\\nfor autonomous driving,” in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit., 2020, pp. 11 618–11 628.\\n[2] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\\ndriving? the kitti vision benchmark suite,” in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit., 2012, pp. 3354–3361.\\n[3] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,\\nJ. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam,\\nH. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi,\\nY. Zhang, J. Shlens, Z.-F. Chen, and D. Anguelov, “Scalability in\\nperception for autonomous driving: Waymo open dataset,” in Proc. IEEE\\nConf. Comput. Vis. Pattern Recognit., 2020, pp. 2443–2451.\\n[4] E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby, and\\nA. Mouzakitis, “A survey on 3d object detection methods for au-\\ntonomous driving applications,” IEEE Transactions on Intelligent Trans-\\nportation Systems, vol. 20, no. 10, pp. 3782–3795, 2019.\\n[5] S. M. Marvasti-Zadeh, L. Cheng, H. Ghanei-Yakhdan, and S. Kasaei,\\n“Deep learning for visual tracking: A comprehensive survey,” IEEE\\nTransactions on Intelligent Transportation Systems, 2021.\\n[6] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and\\nD. Terzopoulos, “Image segmentation using deep learning: A survey,”\\nIEEE transactions on pattern analysis and machine intelligence, vol. PP,\\n2021.\\n[7] Z. MacHardy, A. Khan, K. Obana, and S. Iwashina, “V2x access\\ntechnologies: Regulation, research, and remaining challenges,” IEEE\\nCommunications Surveys & Tutorials, vol. 20, no. 3, pp. 1858–1877,\\n2018.\\n[8] M. Muhammad and G. A. Safdar, “Survey on existing authentication\\nissues for cellular-assisted v2x communication,” Vehicular Communica-\\ntions, vol. 12, pp. 50–65, 2018.\\n[9] M. Hasan, S. Mohan, T. Shimizu, and H. Lu, “Securing vehicle-\\nto-everything (v2x) communication platforms,” IEEE Transactions on\\nIntelligent Vehicles, vol. 5, no. 4, pp. 693–713, 2020.\\n[10] V. Mannoni, V. Berg, S. Sesia, and E. Perraud, “A comparison of the v2x\\ncommunication systems: Its-g5 and c-v2x,” in 2019 IEEE 89th Vehicular\\nTechnology Conference (VTC2019-Spring).\\nIEEE, 2019, pp. 1–5.\\n[11] Q. Chen, S. Tang, Q. Yang, and S. Fu, “Cooper: Cooperative perception\\nfor connected autonomous vehicles based on 3d point clouds,” in\\nIEEE 39th International Conference on Distributed Computing Systems\\n(ICDCS), 2019, pp. 514–524.\\n8\\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. JULY, 2022\\n[12] T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, J. Tu,\\nand R. Urtasun, “V2vnet: Vehicle-to-vehicle communication for joint\\nperception and prediction,” in ECCV, 2020.\\n[13] Y. Li, S. Ren, P. Wu, S. Chen, C. Feng, and W. Zhang, “Learning distilled\\ncollaboration graph for multi-agent perception,” in NeurIPS, 2021.\\n[14] Y. Yuan and M. Sester, “Comap: A synthetic dataset for collective multi-\\nagent perception of autonomous driving,” The International Archives\\nof Photogrammetry, Remote Sensing and Spatial Information Sciences,\\nvol. 43, pp. 255–263, 2021.\\n[15] Y. Yuan, H. Cheng, and M. Sester, “Keypoints-based deep feature fusion\\nfor cooperative vehicle detection of autonomous driving,” IEEE Robotics\\nand Automation Letters, vol. 7, no. 2, pp. 3054–3061, 2022.\\n[16] D. Krajzewicz, J. Erdmann, M. Behrisch, and L. Bieker, “Recent\\ndevelopment and applications of sumo-simulation of urban mobility,”\\nInternational journal on advances in systems and measurements, vol. 5,\\nno. 3&4, 2012.\\n[17] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,\\n“CARLA: An open urban driving simulator,” in Proceedings of the 1st\\nAnnual Conference on Robot Learning, 2017, pp. 1–16.\\n[18] Y.-C. Liu, J. Tian, N. Glaser, and Z. Kira, “When2com: Multi-agent\\nperception via communication graph grouping,” in IEEE Conference\\non Computer Vision and Pattern Recognition (CVPR), 2020, pp. 4105–\\n4114.\\n[19] Y.-C. Liu, J. Tian, C.-Y. Ma, N. Glaser, C.-W. Kuo, and Z. Kira,\\n“Who2com: Collaborative perception via learnable handshake communi-\\ncation,” in IEEE International Conference on Robotics and Automation\\n(ICRA), 2020, pp. 6876–6883.\\n[20] R. Xu, H. Xiang, X. Xia, X. Han, J. Liu, and J. Ma, “Opv2v: An open\\nbenchmark dataset and fusion pipeline for perception with vehicle-to-\\nvehicle communication,” in ICRA, 2022.\\n[21] Z. Xiao, Z. Mo, K. Jiang, and D. Yang, “Multimedia fusion at semantic\\nlevel in vehicle cooperactive perception,” in IEEE International Confer-\\nence on Multimedia & Expo Workshops (ICMEW), 2018, pp. 1–6.\\n[22] Y. Maalej, S. Sorour, A. Abdel-Rahim, and M. Guizani, “Vanets meet\\nautonomous vehicles: A multimodal 3d environment learning approach,”\\nin IEEE Global Communications Conference, 2017, pp. 1–6.\\n[23] E. Arnold, M. Dianati, R. de Temple, and S. Fallah, “Cooperative\\nperception for 3d object detection in driving scenarios using infrastruc-\\nture sensors,” IEEE Transactions on Intelligent Transportation Systems,\\n2020.\\n[24] M. Howe, I. Reid, and J. Mackenzie, “Weakly supervised training of\\nmonocular 3d object detectors using wide baseline multi-view trafﬁc\\ncamera data,” in Brit. Mach. Vis. Conf., 2021.\\n[25] H. Yu, Y. Luo, M. Shu, Y. Huo, Z. Yang, Y. Shi, Z. Guo, H. Li,\\nX. Hu, J. Yuan, and Z. Nie, “Dair-v2x: A large-scale dataset for vehicle-\\ninfrastructure cooperative 3d object detection,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2022.\\n[26] S. Manivasagam, S. Wang, K. Wong, W. Zeng, M. Sazanovich, S. Tan,\\nB. Yang, W.-C. Ma, and R. Urtasun, “Lidarsim: Realistic lidar simulation\\nby leveraging the real world,” in IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), 2020, pp. 11 164–11 173.\\n[27] R. Xu, Y. Guo, X. Han, X. Xia, H. Xiang, and J. Ma, “Opencda:\\nan open cooperative driving automation framework integrated with co-\\nsimulation,” in IEEE International Intelligent Transportation Systems\\nConference (ITSC).\\nIEEE, 2021, pp. 1155–1162.\\n[28] E. Arnold, S. Mozaffari, and M. Dianati, “Fast and robust registration\\nof partially overlapping point clouds,” IEEE Robotics and Automation\\nLetters, 2021.\\n[29] A. Patil, S. Malla, H. Gang, and Y.-T. Chen, “The h3d dataset for\\nfull-surround 3d multi-object detection and tracking in crowded urban\\nscenes,” 2019 International Conference on Robotics and Automation\\n(ICRA), pp. 9552–9557, 2019.\\n[30] M. Pitropov, D. Garcia, J. Rebello, M. Smart, C. Wang, K. Czarnecki,\\nand S. L. Waslander, “Canadian adverse driving conditions dataset,” The\\nInternational Journal of Robotics Research, vol. 40, pp. 681 – 690, 2021.\\n[31] Q.-H. Pham, P. Sevestre, R. Pahwa, H. Zhan, C. H. Pang, Y. Chen,\\nA. Mustafa, V. Chandrasekhar, and J. Lin, “A*3d dataset: Towards\\nautonomous driving in challenging environments,” in IEEE International\\nConference on Robotics and Automation (ICRA), 2020, pp. 2267–2273.\\n[32] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O’Dea,\\nM. Uriar, S. Milz, M. Simon, K. Amende, C. Witt, H. Rashed, S. Chen-\\nnupati, S. Nayak, S. Mansoor, X. Perroton, and P. Perez, “Woodscape: A\\nmulti-task, multi-camera ﬁsheye dataset for autonomous driving,” 2019\\nIEEE/CVF International Conference on Computer Vision (ICCV), pp.\\n9307–9317, 2019.\\n[33] X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin,\\nand R. Yang, “The apolloscape dataset for autonomous driving,” in\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\\nWorkshops (CVPRW), 2018, pp. 1067–10 676.\\n[34] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-\\nson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for\\nsemantic urban scene understanding,” in IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR), 2016, pp. 3213–3223.\\n[35] G. Ros, L. Sellart, J. Materzynska, D. V´azquez, and A. M. L´opez, “The\\nsynthia dataset: A large collection of synthetic images for semantic\\nsegmentation of urban scenes,” in IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), 2016, pp. 3234–3243.\\n[36] G. Neuhold, T. Ollmann, S. R. Bul`o, and P. Kontschieder, “The mapillary\\nvistas dataset for semantic understanding of street scenes,” in IEEE\\nInternational Conference on Computer Vision (ICCV), 2017, pp. 5000–\\n5009.\\n[37] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,\\nand J. Gall, “Semantickitti: A dataset for semantic scene understand-\\ning of lidar sequences,” 2019 IEEE/CVF International Conference on\\nComputer Vision (ICCV), pp. 9296–9306, 2019.\\n[38] Q. Hu, B. Yang, S. Khalid, W. Xiao, A. Trigoni, and A. Markham,\\n“Towards semantic segmentation of urban-scale 3d point clouds: A\\ndataset, benchmarks and challenges,” in IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR), 2021.\\n[39] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. T. Hartnett,\\nD. Wang, P. Carr, S. Lucey, D. Ramanan, and J. Hays, “Argoverse: 3d\\ntracking and forecasting with rich maps,” in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit., 2019, pp. 8740–8749.\\n[40] S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y. Chai,\\nB. Sapp, C. Qi, Y. Zhou, Z. Yang, A. Chouard, P. Sun, J. Ngiam,\\nV. Vasudevan, A. McCauley, J. Shlens, and D. Anguelov, “Large scale\\ninteractive motion forecasting for autonomous driving : The waymo open\\nmotion dataset,” ArXiv, vol. abs/2104.10133, 2021.\\n[41] D. Jia and D. Ngoduy, “Enhanced cooperative car-following trafﬁc\\nmodel with the combination of v2v and v2i communication,” Trans-\\nportation Research Part B-methodological, vol. 90, pp. 172–191, 2016.\\n[42] S.-W. Kim, B. Qin, Z. J. Chong, X. Shen, W. Liu, M. Ang, E. Fraz-\\nzoli, and D. Rus, “Multivehicle cooperative driving using cooperative\\nperception: Design and experimental validation,” IEEE Transactions on\\nIntelligent Transportation Systems, vol. 16, pp. 663–680, 2015.\\n[43] Z. Y. Rawashdeh and Z. Wang, “Collaborative automated driving: A\\nmachine learning-based method to enhance the accuracy of shared\\ninformation,” in International Conference on Intelligent Transportation\\nSystems (ITSC), 2018, pp. 3961–3966.\\n[44] Q. Chen, T. Yuan, J. Hillenbrand, A. Gern, T. Roth, F. Kuhnt, J. M.\\nZ¨ollner, J. Breu, M. Bogdanovic, and C. Weiss, “Dsrc and radar object\\nmatching for cooperative driver assistance systems,” in IEEE Intelligent\\nVehicles Symposium (IV), 2015, pp. 1348–1354.\\n[45] S. Suo, S. Regalado, S. Casas, and R. Urtasun, “Trafﬁcsim: Learning\\nto simulate realistic multi-agent behaviors,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2021, pp. 10 400–10 409.\\n[46] S. Tan, K. Wong, S. Wang, S. Manivasagam, M. Ren, and R. Urtasun,\\n“Scenegen: Learning to generate realistic trafﬁc scenes,” in Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\\ntion, 2021, pp. 892–901.\\n[47] P. Wu, S. Chen, and D. Metaxas, “Motionnet: Joint perception and\\nmotion prediction for autonomous driving based on bird’s eye view\\nmaps,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp.\\n11 382–11 392.\\n[48] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time end-\\nto-end 3d detection, tracking and motion forecasting with a single\\nconvolutional net,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\\n2018, pp. 3569–3577.\\n[49] J. Luiten, A. Osep, P. Dendorfer, P. Torr, A. Geiger, L. Leal-Taix´e,\\nand B. Leibe, “Hota: A higher order metric for evaluating multi-object\\ntracking,” International Journal of Computer Vision, vol. 129, no. 2, p.\\n548–578, Oct 2020.\\n[50] K. Bernardin and R. Stiefelhagen, “Evaluating multiple object tracking\\nperformance: The clear mot metrics.” EURASIP J. Image Video Process.,\\n2008.\\n[51] A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online\\nand realtime tracking,” in 2016 IEEE International Conference on Image\\nProcessing (ICIP), 2016, pp. 3464–3468.\\n[52] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-\\nworks for biomedical image segmentation,” Medical Image Computing\\nand Computer-Assisted Intervention – MICCAI 2015, May 2015.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2=pd.DataFrame(['0',text2]).T\n",
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "2NP1KI4KaP04",
        "outputId": "5833cd6a-1367-44fb-90f9-e2113dbb6498"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0                                                  1\n",
              "0  0  IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c7cb6d9-8a1d-460c-bab8-e2298020c2ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c7cb6d9-8a1d-460c-bab8-e2298020c2ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c7cb6d9-8a1d-460c-bab8-e2298020c2ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c7cb6d9-8a1d-460c-bab8-e2298020c2ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "#df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
        "df2.columns = ['title', 'text']\n",
        "\n",
        "# Tokenize the text and save the number of tokens to a new column\n",
        "df2['n_tokens'] = df2.text.apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "# Visualize the number of tokens in the entire article \n",
        "df2.n_tokens.hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "eBOLYveEaUGz",
        "outputId": "abf51b7c-9c46-43c7-c843-e3bff4282ee2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 500\n",
        "\n",
        "# Function to split the text into chunks of a maximum number of tokens\n",
        "def split_into_many(text, max_tokens = max_tokens):\n",
        "\n",
        "    # Split the text into sentences\n",
        "    sentences = text.split('. ')\n",
        "\n",
        "    # Get the number of tokens for each sentence\n",
        "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
        "    \n",
        "    chunks = []\n",
        "    tokens_so_far = 0\n",
        "    chunk = []\n",
        "\n",
        "    # Loop through the sentences and tokens joined together in a tuple\n",
        "    for sentence, token in zip(sentences, n_tokens):\n",
        "\n",
        "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
        "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
        "        # the chunk and tokens so far\n",
        "        if tokens_so_far + token > max_tokens:\n",
        "            chunks.append(\". \".join(chunk) + \".\")\n",
        "            chunk = []\n",
        "            tokens_so_far = 0\n",
        "\n",
        "        # If the number of tokens in the current sentence is greater than the max number of \n",
        "        # tokens, go to the next sentence\n",
        "        if token > max_tokens:\n",
        "            continue\n",
        "\n",
        "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
        "        chunk.append(sentence)\n",
        "        tokens_so_far += token + 1\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "i5-VmciDZWf1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shortened1 = []\n",
        "shortened2 = []\n",
        "# Loop through the dataframe1\n",
        "for row in df1.iterrows():\n",
        "\n",
        "    # If the text is None, go to the next row\n",
        "    if row[1]['text'] is None:\n",
        "        continue\n",
        "\n",
        "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
        "    if row[1]['n_tokens'] > max_tokens:\n",
        "        shortened1 += split_into_many(row[1]['text'])\n",
        "    \n",
        "    # Otherwise, add the text to the list of shortened texts\n",
        "    else:\n",
        "        shortened1.append( row[1]['text'] )\n",
        "\n",
        "shortened2 = []\n",
        "# Loop through the dataframe2\n",
        "for row in df2.iterrows():\n",
        "\n",
        "    # If the text is None, go to the next row\n",
        "    if row[1]['text'] is None:\n",
        "        continue\n",
        "\n",
        "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
        "    if row[1]['n_tokens'] > max_tokens:\n",
        "        shortened2 += split_into_many(row[1]['text'])\n",
        "    \n",
        "    # Otherwise, add the text to the list of shortened texts\n",
        "    else:\n",
        "        shortened2.append( row[1]['text'] )"
      ],
      "metadata": {
        "id": "4rKfaOU0aCvu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame(shortened1, columns = ['text'])\n",
        "df1['n_tokens'] = df1.text.apply(lambda x: len(tokenizer.encode(x)))\n",
        "df1.n_tokens.hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "3teCd75CFd1l",
        "outputId": "08d3e5c6-d356-4983-a348-b2985a125244"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noted, I combined 2 pdfs first before generating the embeddings. However, I can also generate the embeddings separately and then concatenate them. \n",
        "\n",
        "Considerations: \n",
        "\n",
        "1) If the texts in the two DataFrames are unrelated or represent different contexts, it might be more appropriate to embed them separately and keep their embeddings distinct. \n",
        "\n",
        "2) If the texts are related or you want to capture relationships between them, concatenating the texts and then embedding the combined data may be more suitable.\n",
        "```"
      ],
      "metadata": {
        "id": "NwokzV46plky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame(shortened2, columns = ['text'])\n",
        "df2['n_tokens'] = df2.text.apply(lambda x: len(tokenizer.encode(x)))\n",
        "df2.n_tokens.hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "FG5EHtxTbBX9",
        "outputId": "b6d889ee-095b-4ba7-a494-c66c00595800"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2MblAxNHo4h1",
        "outputId": "e9458788-67f6-41ac-8204-9448f7f8b76d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  n_tokens\n",
              "0  Multi-Robot Scene Completion:\\nTowards Task-Ag...       451\n",
              "1  Since these intermediate features are easy to ...       499\n",
              "2  To address this\\nchallenge, we further design ...       432\n",
              "3  Existing works commonly\\nconsider a speciﬁc do...       391\n",
              "4  SSRL is generally\\ncomposed of: (1) task-agnos...       496"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e3a379a-31c0-4f11-bd22-60beadfb3500\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>n_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multi-Robot Scene Completion:\\nTowards Task-Ag...</td>\n",
              "      <td>451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Since these intermediate features are easy to ...</td>\n",
              "      <td>499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>To address this\\nchallenge, we further design ...</td>\n",
              "      <td>432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Existing works commonly\\nconsider a speciﬁc do...</td>\n",
              "      <td>391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SSRL is generally\\ncomposed of: (1) task-agnos...</td>\n",
              "      <td>496</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e3a379a-31c0-4f11-bd22-60beadfb3500')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1e3a379a-31c0-4f11-bd22-60beadfb3500 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1e3a379a-31c0-4f11-bd22-60beadfb3500');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n",
        "#df1.to_csv('embeddings1.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "soRzqBYYbhOE",
        "outputId": "6e8b4945-1acb-475d-8a5b-48f16084dbbb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  n_tokens  \\\n",
              "0  Multi-Robot Scene Completion:\\nTowards Task-Ag...       451   \n",
              "1  Since these intermediate features are easy to ...       499   \n",
              "2  To address this\\nchallenge, we further design ...       432   \n",
              "3  Existing works commonly\\nconsider a speciﬁc do...       391   \n",
              "4  SSRL is generally\\ncomposed of: (1) task-agnos...       496   \n",
              "\n",
              "                                          embeddings  \n",
              "0  [-0.02197783626616001, -0.00578127708286047, 0...  \n",
              "1  [-0.013097502291202545, -0.0060057369992136955...  \n",
              "2  [-0.028747987002134323, -0.00900447741150856, ...  \n",
              "3  [-0.012662009336054325, 0.0021576671861112118,...  \n",
              "4  [-0.01578858681023121, 0.008497853763401508, 0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7826097-2c04-4174-a752-e1a908e75322\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>n_tokens</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multi-Robot Scene Completion:\\nTowards Task-Ag...</td>\n",
              "      <td>451</td>\n",
              "      <td>[-0.02197783626616001, -0.00578127708286047, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Since these intermediate features are easy to ...</td>\n",
              "      <td>499</td>\n",
              "      <td>[-0.013097502291202545, -0.0060057369992136955...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>To address this\\nchallenge, we further design ...</td>\n",
              "      <td>432</td>\n",
              "      <td>[-0.028747987002134323, -0.00900447741150856, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Existing works commonly\\nconsider a speciﬁc do...</td>\n",
              "      <td>391</td>\n",
              "      <td>[-0.012662009336054325, 0.0021576671861112118,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SSRL is generally\\ncomposed of: (1) task-agnos...</td>\n",
              "      <td>496</td>\n",
              "      <td>[-0.01578858681023121, 0.008497853763401508, 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7826097-2c04-4174-a752-e1a908e75322')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e7826097-2c04-4174-a752-e1a908e75322 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e7826097-2c04-4174-a752-e1a908e75322');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.embeddings_utils import distances_from_embeddings, cosine_similarity\n",
        "def create_context(\n",
        "    question, df, max_len=1800, size=\"ada\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a context for a question by finding the most similar context from the dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the embeddings for the question\n",
        "    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
        "\n",
        "    # Get the distances from the embeddings\n",
        "    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n",
        "\n",
        "\n",
        "    returns = []\n",
        "    cur_len = 0\n",
        "\n",
        "    # Sort by distance and add the text to the context until the context is too long\n",
        "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
        "        \n",
        "        # Add the length of the text to the current length\n",
        "        cur_len += row['n_tokens'] + 4\n",
        "        \n",
        "        # If the context is too long, break\n",
        "        if cur_len > max_len:\n",
        "            break\n",
        "        \n",
        "        # Else add it to the text that is being returned\n",
        "        returns.append(row[\"text\"])\n",
        "\n",
        "    # Return the context\n",
        "    return \"\\n\\n###\\n\\n\".join(returns)"
      ],
      "metadata": {
        "id": "kuMwod1aopNQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(\n",
        "    df,\n",
        "    question=\"How to show my competitiveness to interviewers?\",\n",
        "    max_len=1800,\n",
        "    size=\"ada\",\n",
        "    debug=False,\n",
        "    max_tokens=150,\n",
        "    stop_sequence=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Answer a question based on the most similar context from the dataframe texts\n",
        "    \"\"\"\n",
        "    context = create_context(\n",
        "        question,\n",
        "        df,\n",
        "        max_len=max_len,\n",
        "        size=size,\n",
        "    )\n",
        "    prompt = f\"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
        "\n",
        "    Context:{context}\n",
        "\n",
        "    Q:{question}\n",
        "    A:\"\"\"\n",
        "    # If debug, print the raw model response\n",
        "    if debug:\n",
        "        print(\"Context:\\n\" + context)\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "    try:\n",
        "        # Create a completions using the question and context\n",
        "        response = openai.ChatCompletion.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response['choices'][0][\"message\"][\"content\"]\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "nwPyq-uIqjb9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=answer_question(df, question=\"How is V2X related to collabrative perception and how is multi-robot scene related to collabrative perception?\", debug=False)"
      ],
      "metadata": {
        "id": "StBrXnyiq-Br"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ytER42surGGX",
        "outputId": "e6683903-9c63-4957-cb80-fcf65a33f3e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'V2X communication enables collaboration between vehicles and other entities in the neighboring environment and can fundamentally improve the perception system for autonomous driving through collaborative perception. The development of collaborative perception requires expertise from both the communication and perception communities. The V2X-Sim dataset is a comprehensive simulated multi-agent perception dataset for V2X-aided autonomous driving that supports various perception tasks and multi-modality perception. The dataset includes multi-agent sensor recordings from the roadside unit (RSU) and multiple vehicles that enable collaborative perception in a multi-robot scene. Thus, V2X is related to collaborative perception, and the multi-robot scene is a context for collaborative perception research.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}